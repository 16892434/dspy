{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219211db",
   "metadata": {},
   "source": [
    "# Given example and template, demonstrate how chat adapter works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13235a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import dsp\n",
    "\n",
    "openai.api_key = os.getenv('PERSONAL_OPENAI_API_KEY')\n",
    "openai.api_base = os.getenv('API_BASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1810a047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "try: import google.colab; root_path = 'dsp'\n",
    "except: root_path = '.'\n",
    "    \n",
    "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(root_path, 'cache')\n",
    "\n",
    "openai_key = os.getenv('PERSONAL_OPENAI_API_KEY')  # or replace with your API key (optional)\n",
    "colbert_server = 'http://ec2-44-228-128-229.us-west-2.compute.amazonaws.com:8893/api/search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f61109d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [('Who produced the album that included a re-recording of \"Lithium\"?', ['Butch Vig']),\n",
    "         ('Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?', ['Kevin Greutert']),\n",
    "         ('The heir to the Du Pont family fortune sponsored what wrestling team?', ['Foxcatcher', 'Team Foxcatcher', 'Foxcatcher Team']),\n",
    "         ('In what year was the star of To Hell and Back born?', ['1925']),\n",
    "         ('Which award did the first book of Gary Zukav receive?', ['U.S. National Book Award', 'National Book Award']),\n",
    "         ('What city was the victim of Joseph Druces working in?', ['Boston, Massachusetts', 'Boston']),]\n",
    "\n",
    "# Train is an array of examples\n",
    "train = [dsp.Example(question=question, answer=answer) for question, answer in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cafcba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = [('Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?', ['E. L. Doctorow', 'E.L. Doctorow', 'Doctorow']),\n",
    "       ('What documentary about the Gilgo Beach Killer debuted on A&E?', ['The Killing Season']),\n",
    "       ('Right Back At It Again contains lyrics co-written by the singer born in what city?', ['Gainesville, Florida', 'Gainesville']),\n",
    "       ('What year was the party of the winner of the 1971 San Francisco mayoral election founded?', ['1828']),\n",
    "       ('Which author is English: John Braine or Studs Terkel?', ['John Braine']),\n",
    "       ('Anthony Dirrell is the brother of which super middleweight title holder?', ['Andre Dirrell']),\n",
    "       ('In which city is the sports nutrition business established by Oliver Cookson based ?', ['Cheshire', 'Cheshire, UK']),\n",
    "       ('Find the birth date of the actor who played roles in First Wives Club and Searching for the Elephant.', ['February 13, 1980']),\n",
    "       ('Kyle Moran was born in the town on what river?', ['Castletown', 'Castletown River']),\n",
    "       (\"What is the name of one branch of Robert D. Braun's speciality?\", ['aeronautical engineering', 'astronautical engineering', 'aeronautics', 'astronautics']),\n",
    "       (\"Where was the actress who played the niece in the Priest film born?\", ['Surrey', 'Guildford, Surrey']),\n",
    "       ('Name the movie in which the daughter of Noel Harrison plays Violet Trefusis.', ['Portrait of a Marriage']),\n",
    "       ('What year was the father of the Princes in the Tower born?', ['1442'])]\n",
    "\n",
    "dev = [dsp.Example(question=question, answer=answer) for question, answer in dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a9107",
   "metadata": {},
   "source": [
    "Basic Adapter Input: Template with Example\n",
    "\n",
    "Basic Adapter Output: Prompt String\n",
    "\n",
    "Chat Adapter Input: Template with Example\n",
    "\n",
    "Chat Adapter Output: Prompt Dictionairy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab0e23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template, Demos, and Question Construction\n",
    "Question = dsp.Type(prefix=\"Question:\", desc=\"${the question to be answered}\")\n",
    "Answer = dsp.Type(prefix=\"Answer:\", desc=\"${a short factoid answer, often between 1 and 5 words}\", format=dsp.format_answers)\n",
    "qa_template = dsp.Template(instructions=\"Answer questions with short factoid answers.\", question=Question(), answer=Answer())\n",
    "\n",
    "question = dev[0].question\n",
    "demos = dsp.sample(train, k=1)\n",
    "training_demos = dsp.sample(train, k=4)\n",
    "example = dsp.Example(question=question, demos=demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579170f",
   "metadata": {},
   "source": [
    "### Load Models and Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9109feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = dsp.ColBERTv2(url=colbert_server)\n",
    "davinci = dsp.GPT3(model='text-davinci-002', api_key=openai_key)\n",
    "turbo = dsp.GPT3(model='gpt-3.5-turbo', api_key=openai_key, model_type=\"chat\")\n",
    "\n",
    "\n",
    "basic_adapter = dsp.modules.BasicAdapter()\n",
    "chat_adapter = dsp.modules.ChatAdapter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcb15e",
   "metadata": {},
   "source": [
    "## Basic Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6847a05",
   "metadata": {},
   "source": [
    "### Basic Adapter with demos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4204f54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "PARTS\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "PROMPT\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Which award did the first book of Gary Zukav receive?\n",
      "Answer: U.S. National Book Award\n",
      "\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=davinci, rm=rm)\n",
    "\n",
    "parts = qa_template(example)\n",
    "prompt = basic_adapter(parts)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PARTS\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PROMPT\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c3ff3",
   "metadata": {},
   "source": [
    "### Chat Adapter with demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccfba721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'user', 'content': 'Answer questions with short factoid answers.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question, demos=demos)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8753768",
   "metadata": {},
   "source": [
    "### Basic Adapter with no demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6552e0ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "PARTS\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "[]\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "PROMPT\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "\n",
      "---\n",
      "\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=davinci, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question)\n",
    "parts = qa_template(example)\n",
    "prompt = basic_adapter(parts)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PARTS\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PROMPT\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4089048b",
   "metadata": {},
   "source": [
    "### Chat Adapter with no demos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4b9f51d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "PARTS\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "[]\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "PROMPT\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Answer questions with short factoid answers.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nQuestion: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}]}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PARTS\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PROMPT\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38d034",
   "metadata": {},
   "source": [
    "### Basic Adapter with demos and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c6b7a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Context = dsp.Type(\n",
    "    prefix=\"Context:\\n\",\n",
    "    desc=\"${sources that may contain relevant content}\",\n",
    "    format=dsp.passages2text\n",
    ")\n",
    "\n",
    "qa_template_with_passages = dsp.Template(\n",
    "    instructions=qa_template.instructions,\n",
    "    context=Context(), question=Question(), answer=Answer()\n",
    ")\n",
    "\n",
    "passages = dsp.retrieve(question, k=1)\n",
    "example = dsp.Example(question=question, context=passages, demos=demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "465c9778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "PASSAGES\n",
      "0:\n",
      "Julia Peterkin | Julia Peterkin Julia Peterkin (October 31, 1880 – August 10, 1961) was an American author from South Carolina. In 1929 she won the Pulitzer Prize for Novel/Literature, for her novel \"Scarlet Sister Mary.\" She wrote several novels about the plantation South, especially the Gullah people of the Low Country. She was one of the few white authors who wrote about the African-American experience. Julia Mood was born in Laurens County, South Carolina. Her father was a physician, and she was the third of his four children. Her mother died soon after her birth, and her father later married Janie Brogdon.\n",
      "------\n",
      "PARTS\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Context:\n",
      "${sources that may contain relevant content}\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Context:\n",
      "«Julia Peterkin | Julia Peterkin Julia Peterkin (October 31, 1880 – August 10, 1961) was an American author from South Carolina. In 1929 she won the Pulitzer Prize for Novel/Literature, for her novel \"Scarlet Sister Mary.\" She wrote several novels about the plantation South, especially the Gullah people of the Low Country. She was one of the few white authors who wrote about the African-American experience. Julia Mood was born in Laurens County, South Carolina. Her father was a physician, and she was the third of his four children. Her mother died soon after her birth, and her father later married Janie Brogdon.»\n",
      "\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "\n",
      "Answer: \n",
      "long_query:\n",
      "True\n",
      "------\n",
      "PROMPT\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Question: Which award did the first book of Gary Zukav receive?\n",
      "Answer: U.S. National Book Award\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context:\n",
      "${sources that may contain relevant content}\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "«Julia Peterkin | Julia Peterkin Julia Peterkin (October 31, 1880 – August 10, 1961) was an American author from South Carolina. In 1929 she won the Pulitzer Prize for Novel/Literature, for her novel \"Scarlet Sister Mary.\" She wrote several novels about the plantation South, especially the Gullah people of the Low Country. She was one of the few white authors who wrote about the African-American experience. Julia Mood was born in Laurens County, South Carolina. Her father was a physician, and she was the third of his four children. Her mother died soon after her birth, and her father later married Janie Brogdon.»\n",
      "\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=davinci, rm=rm)\n",
    "parts = qa_template_with_passages(example)\n",
    "prompt = basic_adapter(parts)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PASSAGES\")\n",
    "for i, v in enumerate(passages):\n",
    "    print(str(i)+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PARTS\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PROMPT\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774400c",
   "metadata": {},
   "source": [
    "### Chat Adapter with demos and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "649dee23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "PASSAGES\n",
      "0:\n",
      "Julia Peterkin | Julia Peterkin Julia Peterkin (October 31, 1880 – August 10, 1961) was an American author from South Carolina. In 1929 she won the Pulitzer Prize for Novel/Literature, for her novel \"Scarlet Sister Mary.\" She wrote several novels about the plantation South, especially the Gullah people of the Low Country. She was one of the few white authors who wrote about the African-American experience. Julia Mood was born in Laurens County, South Carolina. Her father was a physician, and she was the third of his four children. Her mother died soon after her birth, and her father later married Janie Brogdon.\n",
      "------\n",
      "PARTS\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Context:\n",
      "${sources that may contain relevant content}\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Context:\n",
      "«Julia Peterkin | Julia Peterkin Julia Peterkin (October 31, 1880 – August 10, 1961) was an American author from South Carolina. In 1929 she won the Pulitzer Prize for Novel/Literature, for her novel \"Scarlet Sister Mary.\" She wrote several novels about the plantation South, especially the Gullah people of the Low Country. She was one of the few white authors who wrote about the African-American experience. Julia Mood was born in Laurens County, South Carolina. Her father was a physician, and she was the third of his four children. Her mother died soon after her birth, and her father later married Janie Brogdon.»\n",
      "\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "\n",
      "Answer: \n",
      "long_query:\n",
      "True\n",
      "------\n",
      "PROMPT\n",
      "gpt-3.5-turbo\n",
      "messages\n",
      "0:\n",
      "{'role': 'user', 'content': 'Answer questions with short factoid answers.\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nContext:\\n«Julia Peterkin | Julia Peterkin Julia Peterkin (October 31, 1880 – August 10, 1961) was an American author from South Carolina. In 1929 she won the Pulitzer Prize for Novel/Literature, for her novel \"Scarlet Sister Mary.\" She wrote several novels about the plantation South, especially the Gullah people of the Low Country. She was one of the few white authors who wrote about the African-American experience. Julia Mood was born in Laurens County, South Carolina. Her father was a physician, and she was the third of his four children. Her mother died soon after her birth, and her father later married Janie Brogdon.»\\n\\nQuestion: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\n\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "parts = qa_template_with_passages(example)\n",
    "prompt = chat_adapter(parts)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PASSAGES\")\n",
    "for i, v in enumerate(passages):\n",
    "    print(str(i)+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PARTS\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"PROMPT\")\n",
    "print(prompt['model'])\n",
    "print(\"messages\")\n",
    "for i, v in enumerate(prompt['messages']):\n",
    "    print(str(i)+\":\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116b0e4",
   "metadata": {},
   "source": [
    "# Program 1: Vanilla GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "635800d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.evaluation.utils import evaluate\n",
    "\n",
    "def vanilla_LM_QA(q: str) -> str:\n",
    "    e = dsp.Example(question=q, demos=training_demos)\n",
    "    e, completions = dsp.generate(qa_template)(e, stage='qa')\n",
    "    return completions.answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eee16d",
   "metadata": {},
   "source": [
    "## Testing Chat Adapter Flags with demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a8cfc",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=True, multi_turn=True, strict_turn=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e80f347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'system', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}'}\n",
      "{'role': 'assistant', 'content': 'Got it.'}\n",
      "{'role': 'user', 'content': 'Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award'}\n",
      "{'role': 'assistant', 'content': 'Got it.'}\n",
      "{'role': 'user', 'content': 'Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question, demos=demos)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=True, multi_turn=True, strict_turn=True)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfeb80b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Invalid key (or out of budget).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvanilla_LM_QA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m, lm\u001b[38;5;241m.\u001b[39minspect_history(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m, in \u001b[0;36mvanilla_LM_QA\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvanilla_LM_QA\u001b[39m(q: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      4\u001b[0m     e \u001b[38;5;241m=\u001b[39m dsp\u001b[38;5;241m.\u001b[39mExample(question\u001b[38;5;241m=\u001b[39mq, demos\u001b[38;5;241m=\u001b[39mtraining_demos)\n\u001b[0;32m----> 5\u001b[0m     e, completions \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa_template\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completions\u001b[38;5;241m.\u001b[39manswer\n",
      "File \u001b[0;32m~/programming/dsp/dsp/primitives/predict.py:78\u001b[0m, in \u001b[0;36m_generate.<locals>.do_generate\u001b[0;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Generate and extract the fields.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m prompt \u001b[38;5;241m=\u001b[39m adapter(template(example))\n\u001b[0;32m---> 78\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[Example] \u001b[38;5;241m=\u001b[39m [template\u001b[38;5;241m.\u001b[39mextract(example, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Find the completions that are most comple\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/gpt3.py:321\u001b[0m, in \u001b[0;36mGPT3.__call__\u001b[0;34m(self, prompt, only_completed, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m}\n\u001b[0;32m--> 321\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m choices \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    324\u001b[0m completed_choices \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m choices \u001b[38;5;28;01mif\u001b[39;00m c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinish_reason\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/gpt3.py:287\u001b[0m, in \u001b[0;36mGPT3.request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/gpt3.py:261\u001b[0m, in \u001b[0;36mGPT3.basic_request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    258\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m    260\u001b[0m     }\n\u001b[0;32m--> 261\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcached_gpt3_turbo_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/cache_utils.py:16\u001b[0m, in \u001b[0;36mnoop_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/gpt3.py:377\u001b[0m, in \u001b[0;36m_cached_gpt3_turbo_request_v2_wrapped\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m cache_turn_on \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;129m@NotebookCacheMemory\u001b[39m\u001b[38;5;241m.\u001b[39mcache\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cached_gpt3_turbo_request_v2_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OpenAIObject:\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cached_gpt3_turbo_request_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/memory.py:594\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/memory.py:537\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    534\u001b[0m         must_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m must_call:\n\u001b[0;32m--> 537\u001b[0m     out, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmap_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;66;03m# Memmap the output at the first call to be consistent with\u001b[39;00m\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;66;03m# later calls\u001b[39;00m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/memory.py:779\u001b[0m, in \u001b[0;36mMemorizedFunc.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mprint\u001b[39m(format_call(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, args, kwargs))\n\u001b[0;32m--> 779\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_backend\u001b[38;5;241m.\u001b[39mdump_item(\n\u001b[1;32m    781\u001b[0m     [func_id, args_id], output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose)\n\u001b[1;32m    783\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/gpt3.py:371\u001b[0m, in \u001b[0;36m_cached_gpt3_turbo_request_v2\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    370\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(OpenAIObject, \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Invalid key (or out of budget)."
     ]
    }
   ],
   "source": [
    "vanilla_LM_QA(dev[0].question), lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72c4f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                               | 0/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "Invalid key (or out of budget).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvanilla_LM_QA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/dsp/dsp/evaluation/utils.py:63\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(fn, dev, metric)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(dev):\n\u001b[1;32m     62\u001b[0m     question \u001b[38;5;241m=\u001b[39m example\u001b[38;5;241m.\u001b[39mquestion\n\u001b[0;32m---> 63\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(example)\n\u001b[1;32m     67\u001b[0m     pred \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;66;03m#.answer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m, in \u001b[0;36mvanilla_LM_QA\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvanilla_LM_QA\u001b[39m(q: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      4\u001b[0m     e \u001b[38;5;241m=\u001b[39m dsp\u001b[38;5;241m.\u001b[39mExample(question\u001b[38;5;241m=\u001b[39mq, demos\u001b[38;5;241m=\u001b[39mtraining_demos)\n\u001b[0;32m----> 5\u001b[0m     e, completions \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqa_template\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completions\u001b[38;5;241m.\u001b[39manswer\n",
      "File \u001b[0;32m~/programming/dsp/dsp/primitives/predict.py:78\u001b[0m, in \u001b[0;36m_generate.<locals>.do_generate\u001b[0;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Generate and extract the fields.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m prompt \u001b[38;5;241m=\u001b[39m adapter(template(example))\n\u001b[0;32m---> 78\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[Example] \u001b[38;5;241m=\u001b[39m [template\u001b[38;5;241m.\u001b[39mextract(example, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Find the completions that are most comple\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/gpt3.py:321\u001b[0m, in \u001b[0;36mGPT3.__call__\u001b[0;34m(self, prompt, only_completed, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m}\n\u001b[0;32m--> 321\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m choices \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    324\u001b[0m completed_choices \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m choices \u001b[38;5;28;01mif\u001b[39;00m c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinish_reason\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/gpt3.py:287\u001b[0m, in \u001b[0;36mGPT3.request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/gpt3.py:261\u001b[0m, in \u001b[0;36mGPT3.basic_request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(prompt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    258\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m    260\u001b[0m     }\n\u001b[0;32m--> 261\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcached_gpt3_turbo_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/cache_utils.py:16\u001b[0m, in \u001b[0;36mnoop_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/gpt3.py:377\u001b[0m, in \u001b[0;36m_cached_gpt3_turbo_request_v2_wrapped\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m cache_turn_on \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;129m@NotebookCacheMemory\u001b[39m\u001b[38;5;241m.\u001b[39mcache\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cached_gpt3_turbo_request_v2_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OpenAIObject:\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cached_gpt3_turbo_request_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/memory.py:594\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/memory.py:537\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    534\u001b[0m         must_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m must_call:\n\u001b[0;32m--> 537\u001b[0m     out, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmap_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;66;03m# Memmap the output at the first call to be consistent with\u001b[39;00m\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;66;03m# later calls\u001b[39;00m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/memory.py:779\u001b[0m, in \u001b[0;36mMemorizedFunc.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mprint\u001b[39m(format_call(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, args, kwargs))\n\u001b[0;32m--> 779\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_backend\u001b[38;5;241m.\u001b[39mdump_item(\n\u001b[1;32m    781\u001b[0m     [func_id, args_id], output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose)\n\u001b[1;32m    783\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/programming/dsp/dsp/modules/gpt3.py:371\u001b[0m, in \u001b[0;36m_cached_gpt3_turbo_request_v2\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    370\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(OpenAIObject, \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Invalid key (or out of budget)."
     ]
    }
   ],
   "source": [
    "evaluate(vanilla_LM_QA, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e14d3e",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=True, multi_turn=True, strict_turn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bde061cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'system', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}'}\n",
      "{'role': 'user', 'content': 'Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award'}\n",
      "{'role': 'user', 'content': 'Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question, demos=demos)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=True, multi_turn=True, strict_turn=False)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a56bb8b",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=True, multi_turn=False, strict_turn=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4b2ee73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'system', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question, demos=demos)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=True, multi_turn=False, strict_turn=True)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d9619",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=True, multi_turn=False, strict_turn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f0cbdc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'system', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question, demos=demos)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=True, multi_turn=False, strict_turn=False)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8361b",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=False, multi_turn=True, strict_turn=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab098ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'user', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'assistant', 'content': 'Got it.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}'}\n",
      "{'role': 'assistant', 'content': 'Got it.'}\n",
      "{'role': 'user', 'content': 'Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award'}\n",
      "{'role': 'assistant', 'content': 'Got it.'}\n",
      "{'role': 'user', 'content': 'Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question, demos=demos)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=False, multi_turn=True, strict_turn=True)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5824fdb",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=False, multi_turn=True, strict_turn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3be0b80b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'user', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}'}\n",
      "{'role': 'user', 'content': 'Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award'}\n",
      "{'role': 'user', 'content': 'Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question, demos=demos)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=False, multi_turn=True, strict_turn=False)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972dcbbe",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=False, multi_turn=False, strict_turn=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f19b3931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'user', 'content': 'Answer questions with short factoid answers.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question, demos=demos)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=False, multi_turn=False, strict_turn=True)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847aec2",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=False, multi_turn=False, strict_turn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db650cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'user', 'content': 'Answer questions with short factoid answers.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question, demos=demos)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=False, multi_turn=False, strict_turn=False)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8eac8d",
   "metadata": {},
   "source": [
    "## Testing Chat Adapter Flags with no demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce5865b",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=True, multi_turn=True, strict_turn=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09bf55aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "[]\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'system', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}'}\n",
      "{'role': 'assistant', 'content': 'Got it.'}\n",
      "{'role': 'user', 'content': 'Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=True, multi_turn=True, strict_turn=True)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35523e9e",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=True, multi_turn=True, strict_turn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c06f1567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "[]\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'system', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}'}\n",
      "{'role': 'user', 'content': 'Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=True, multi_turn=True, strict_turn=False)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fad687",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=True, multi_turn=False, strict_turn=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74a97cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "[]\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'system', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nQuestion: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=True, multi_turn=False, strict_turn=True)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e1ccb7",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=True, multi_turn=False, strict_turn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5413c942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "[]\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'system', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nQuestion: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=True, multi_turn=False, strict_turn=False)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a71e2",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=False, multi_turn=True, strict_turn=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc770be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "[]\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'user', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'assistant', 'content': 'Got it.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}'}\n",
      "{'role': 'assistant', 'content': 'Got it.'}\n",
      "{'role': 'user', 'content': 'Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=False, multi_turn=True, strict_turn=True)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9879a7",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=False, multi_turn=True, strict_turn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68c6452d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "[]\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'user', 'content': 'Answer questions with short factoid answers.'}\n",
      "{'role': 'user', 'content': 'Follow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}'}\n",
      "{'role': 'user', 'content': 'Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=False, multi_turn=True, strict_turn=False)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d33af",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=False, multi_turn=False, strict_turn=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b48641f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'user', 'content': 'Answer questions with short factoid answers.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question, demos=demos)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=False, multi_turn=False, strict_turn=True)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882e0f2",
   "metadata": {},
   "source": [
    "### Chat Adapter with system_turn=False, multi_turn=False, strict_turn=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23340ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "parts\n",
      "instructions:\n",
      "Answer questions with short factoid answers.\n",
      "guidelines:\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "rdemos:\n",
      "['Question: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award']\n",
      "ademos:\n",
      "[]\n",
      "query:\n",
      "Question: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\n",
      "Answer: \n",
      "long_query:\n",
      "False\n",
      "------\n",
      "prompt\n",
      "model:\n",
      "gpt-3.5-turbo\n",
      "messages:\n",
      "{'role': 'user', 'content': 'Answer questions with short factoid answers.\\n\\n---\\n\\nFollow the following format.\\n\\nQuestion: ${the question to be answered}\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: Who has a broader scope of profession: E. L. Doctorow or Julia Peterkin?\\nAnswer:'}\n"
     ]
    }
   ],
   "source": [
    "dsp.settings.configure(lm=turbo, rm=rm)\n",
    "\n",
    "example = dsp.Example(question=question, demos=demos)\n",
    "parts = qa_template(example)\n",
    "prompt = chat_adapter(parts, system_turn=False, multi_turn=False, strict_turn=False)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"parts\")\n",
    "for k, v in parts.items():\n",
    "    print(k+\":\")\n",
    "    print(v)\n",
    "\n",
    "print(\"------\")\n",
    "print(\"prompt\")\n",
    "for k, v in prompt.items():\n",
    "    print(k+\":\")\n",
    "    if k==\"messages\":\n",
    "        for turn in v:\n",
    "            print(turn)\n",
    "    else:\n",
    "        print(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
