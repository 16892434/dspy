{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import tiktoken\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Goal is to take a sequence of tokens, and return the topk tokens at position i\n",
    "# Such that the tokens maximize the negative gradient of the loss function across the entire sequence\n",
    "\n",
    "# Initial sequence\n",
    "\n",
    "def setup_model_and_tokenizer():\n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "    chat_template = \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\\\'s questions.' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 %}{{ system_message }}{% endif %}{% if message['role'] == 'user' %}{{ ' USER: ' + message['content'].strip() }}{% elif message['role'] == 'assistant' %}{{ ' ASSISTANT: ' + message['content'].strip() + eos_token }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ' ASSISTANT:' }}{% endif %}\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, chat_template=chat_template)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        \n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdfef17498144668e3e1cbb0aae313a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dspy-propane/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/dspy-propane/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/dspy-propane/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/dspy-propane/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "model, tokenizer = setup_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gradient_tokens(model, tokenizer, sequence, position, top_k=5):\n",
    "#     # Tokenize the input sequence\n",
    "#     messages = [{\"role\": \"user\", \"content\": sequence}]\n",
    "#     inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "#     input_ids = inputs.to(model.device)\n",
    "    \n",
    "#     # Create a copy of input_ids that requires gradient\n",
    "#     input_embeds = model.get_input_embeddings()(input_ids)\n",
    "#     input_embeds.requires_grad_(True)\n",
    "#     input_embeds.retain_grad()\n",
    "    \n",
    "#     # Forward pass\n",
    "#     outputs = model(inputs_embeds=input_embeds)\n",
    "#     logits = outputs.logits\n",
    "    \n",
    "#     # Calculate loss (we'll use the current token prediction loss)\n",
    "#     # Don't shift the logits/labels since we want current token prediction\n",
    "#     loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "#                           input_ids.view(-1))\n",
    "    \n",
    "#     # Calculate gradients\n",
    "#     loss.backward()\n",
    "    \n",
    "#     # Get gradients at the specified position\n",
    "#     position_gradients = input_embeds.grad[0, position]\n",
    "    \n",
    "#     # Get the token embeddings\n",
    "#     token_embeddings = model.get_input_embeddings().weight\n",
    "    \n",
    "#     # Calculate similarity between position gradients and all token embeddings\n",
    "#     similarities = torch.matmul(token_embeddings, position_gradients)\n",
    "    \n",
    "#     # Get top-k tokens that maximize the negative gradient\n",
    "#     top_values, top_indices = torch.topk(-similarities, k=top_k)\n",
    "    \n",
    "#     # Convert token ids to tokens\n",
    "#     top_tokens = [tokenizer.decode([idx.item()]) for idx in top_indices]\n",
    "    \n",
    "#     return top_tokens, list(map(lambda x: -x, top_values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sequence = \"What is the capital of France?\"\n",
    "\n",
    "# print(f\"Analyzing sequence: {sequence}\")\n",
    "\n",
    "# print(\"Converting to chatml format\")\n",
    "# messages = [{\"role\": \"user\", \"content\": sequence}]\n",
    "# inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "# # Get tokenized sequence length\n",
    "# tokens = tokenizer.encode(inputs, return_tensors=\"pt\")[0]\n",
    "# seq_length = len(tokens)\n",
    "\n",
    "# print(f\"\\nTokenized sequence length: {seq_length}\")\n",
    "# print(\"Analyzing gradient-based importance for each position...\")\n",
    "# # only change the tokens that are a part of the prompt\n",
    "# prompt_tokens = tokenizer.encode(sequence, return_tensors=\"pt\")[0]\n",
    "# # Analyze each position\n",
    "# # NOTE: The original GCG paper randomly selects between all positions and takes a uniform sample between the top k replacements\n",
    "# # We also need to limit this to only adjusting the task prompt tokens, not the chatml format or eventually the \"generated\" tokens\n",
    "# for i in range(seq_length - 1):\n",
    "#     tokens_at_pos = tokenizer.decode([tokens[i]])\n",
    "#     print(f\"\\nPosition {i} (current token: '{tokens_at_pos}')\")\n",
    "    \n",
    "#     top_tokens, importance_scores = get_gradient_tokens(model, tokenizer, sequence, i)\n",
    "    \n",
    "#     print(\"Top replacement tokens by gradient magnitude:\")\n",
    "#     for token, score in zip(top_tokens, importance_scores):\n",
    "#         print(f\"  Token: '{token}', Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "import dspy\n",
    "import json\n",
    "from dspy.datasets import DataLoader\n",
    "alpaca_dataset = DataLoader().from_huggingface(\"tatsu-lab/alpaca_eval\",  trust_remote_code=True)\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "import concurrent.futures\n",
    "client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "prompts = [x[\"instruction\"] for x in alpaca_dataset[\"eval\"][:100]]\n",
    "\n",
    "\n",
    "# We are going to generate N sample responses for each prompt using gpt-4o-mini\n",
    "def generate_responses(prompt, N, max_workers=10, good_system_prompt=True):\n",
    "    max_workers = min(max_workers, N)\n",
    "    system_prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\" if good_system_prompt else \"Below is an instruction that describes a task. Write a response that makes grammatical sense, but semantically makes no sense.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    responses = []\n",
    "    def generate_response(*args):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=1.5,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        responses = list(executor.map(generate_response, range(N)))\n",
    "    return responses\n",
    "\n",
    "GENERATE_RESULTS = False\n",
    "if GENERATE_RESULTS:\n",
    "    final_results = {}\n",
    "    for prompt in prompts:\n",
    "        final_results[prompt] = generate_responses(prompt, 10)\n",
    "\n",
    "    import json\n",
    "    with open(\"alpaca_results.json\", \"w\") as f:\n",
    "        json.dump(final_results, f)\n",
    "else:\n",
    "    with open(\"alpaca_results.json\", \"r\") as f:\n",
    "        final_results = json.load(f)\n",
    "    with open(\"alpaca_results_bad.json\", \"r\") as f:\n",
    "        final_results_bad = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# def compute_conditional_probability(model, tokenizer, messages):\n",
    "#     \"\"\"\n",
    "#     Compute the conditional probability of the assistant's reply given the conversation history.\n",
    "\n",
    "#     Args:\n",
    "#         model: Pretrained language model (AutoModelForCausalLM).\n",
    "#         tokenizer: Corresponding tokenizer (AutoTokenizer).\n",
    "#         messages (list): List of messages in the conversation, including the assistant's reply as the last message.\n",
    "#             Each message is a dict with keys 'role' and 'content'.\n",
    "\n",
    "#     Returns:\n",
    "#         total_prob (float): The total conditional probability of the assistant's reply.\n",
    "#         total_log_prob (float): The total log probability.\n",
    "#     \"\"\"\n",
    "#     # Apply chat template to the full conversation (including assistant's reply)\n",
    "#     formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "#     input_ids = tokenizer.encode(formatted_text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "#     # Apply chat template to the prompt messages (excluding assistant's reply)\n",
    "#     prompt_messages = messages[:-1]\n",
    "#     formatted_prompt_text = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "#     prompt_input_ids = tokenizer.encode(formatted_prompt_text, add_special_tokens=False, return_tensors=\"pt\")[0].to(model.device)\n",
    "#     prompt_length = len(prompt_input_ids)\n",
    "\n",
    "#     # Get model outputs\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids)\n",
    "#         logits = outputs.logits  # Shape: [batch_size, seq_len, vocab_size]\n",
    "#         # print(\"logits\", logits.shape)\n",
    "\n",
    "#     logits = logits[:, :-1, :]  # Exclude the last token's logits\n",
    "#     logits = logits.squeeze(0)  # Shape: [seq_len - 1, vocab_size]\n",
    "\n",
    "#     # Target tokens are the input_ids shifted to the left\n",
    "#     target_tokens = input_ids.squeeze(0)[1:]  # Exclude the first token\n",
    "\n",
    "#     # Compute probabilities\n",
    "#     probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "#     # Indices for continuation tokens\n",
    "#     continuation_start = prompt_length - 1  # Since target_tokens excludes the first token\n",
    "#     continuation_end = len(target_tokens)\n",
    "\n",
    "#     # Handle the case where the assistant's reply is empty\n",
    "#     if continuation_start >= continuation_end:\n",
    "#         print(\"WARNING: Assistant's reply is empty\")\n",
    "#         total_log_prob = 0.0\n",
    "#         total_prob = 1.0\n",
    "#         return total_prob, total_log_prob\n",
    "\n",
    "#     # Extract log probabilities for continuation tokens\n",
    "#     continuation_probs = probs[continuation_start:continuation_end, :]\n",
    "#     continuation_target_tokens = target_tokens[continuation_start:continuation_end]\n",
    "\n",
    "#     # Get log probabilities for each continuation token\n",
    "#     token_probs = continuation_probs[\n",
    "#         torch.arange(len(continuation_target_tokens)), continuation_target_tokens\n",
    "#     ]\n",
    "    \n",
    "\n",
    "#     # Sum log probabilities\n",
    "#     total_log_prob = token_probs.sum().item()\n",
    "#     total_prob = torch.exp(torch.tensor(total_log_prob))\n",
    "\n",
    "#     return total_prob.item(), total_log_prob\n",
    "\n",
    "# def compute_loss(model, tokenizer, prompt_messages, continuations):\n",
    "#     \"\"\"\n",
    "#     Compute the loss over a list of assistant replies.\n",
    "\n",
    "#     Args:\n",
    "#         model: Pretrained language model (AutoModelForCausalLM).\n",
    "#         tokenizer: Corresponding tokenizer (AutoTokenizer).\n",
    "#         prompt_messages (list): List of messages representing the conversation history (excluding assistant's reply).\n",
    "#         continuations (list of str): List of assistant replies.\n",
    "\n",
    "#     Returns:\n",
    "#         loss (float): The computed loss value.\n",
    "#     \"\"\"\n",
    "#     total_log_prob = 0.0\n",
    "#     n = len(continuations)\n",
    "\n",
    "#     for continuation in continuations:\n",
    "#         # Build the full conversation including the assistant's reply\n",
    "#         messages = prompt_messages + [{'role': 'assistant', 'content': continuation}]\n",
    "#         _, prob = compute_conditional_probability(model, tokenizer, messages)\n",
    "#         log_prob = torch.log(prob)\n",
    "#         total_log_prob += log_prob\n",
    "\n",
    "#     loss = -total_log_prob / n\n",
    "#     return loss\n",
    "\n",
    "# print(\"Good Results\")\n",
    "# log_probs = []\n",
    "# for prompt in final_results:\n",
    "#     for continuation in final_results[prompt]:\n",
    "#         messages = get_message_format(prompt, continuation)\n",
    "#         prob, log_prob = compute_conditional_probability(model, tokenizer, messages)\n",
    "#         # print(f\"Conditional Probability: {prob}\")\n",
    "#         # print(f\"Log Probability: {log_prob}\")\n",
    "#         log_probs.append(log_prob)\n",
    "# print(f\"Average Log Probability: {sum(log_probs) / len(log_probs)}\")\n",
    "# # Assistant's reply\n",
    "# # continuation = ' in a land far, far away'\n",
    "# print(\"Bad Results\")\n",
    "# log_probs = []\n",
    "# for prompt in final_results_bad:\n",
    "#     for continuation in final_results_bad[prompt]:\n",
    "#         messages = get_message_format(prompt, continuation)\n",
    "#         prob, log_prob = compute_conditional_probability(model, tokenizer, messages)\n",
    "#         # print(f\"Conditional Probability: {prob}\")\n",
    "#         # print(f\"Log Probability: {log_prob}\")\n",
    "#         log_probs.append(log_prob)\n",
    "\n",
    "# print(f\"Average Log Probability: {sum(log_probs) / len(log_probs)}\")\n",
    "# # Compute conditional probability\n",
    "# # messages = prompt_messages + [{'role': 'assistant', 'content': continuation}]\n",
    "# # prob, log_prob = compute_conditional_probability(model, tokenizer, messages)\n",
    "# # print(f\"Conditional Probability: {prob}\")\n",
    "# # print(f\"Log Probability: {log_prob}\")\n",
    "\n",
    "# # Compute loss over multiple continuations\n",
    "# # calculate average conditional probability for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subarray_index(main_list, subarray):\n",
    "    \"\"\"\n",
    "    Find the starting index of a subarray in a main list.\n",
    "\n",
    "    Args:\n",
    "        main_list (list): The larger list.\n",
    "        subarray (list): The subarray to search for.\n",
    "\n",
    "    Returns:\n",
    "        int: The starting index of the subarray in the main list, or -1 if not found.\n",
    "    \"\"\"\n",
    "    n, m = len(main_list), len(subarray)\n",
    "    for i in range(n - m + 1):\n",
    "        if main_list[i:i + m] == subarray:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "Conditional Probability: 4.414557190043943e-27\n",
      "Log Probability: -191.34375\n",
      "Top K Tokens per Position in the Prompt with Greatest Gradients:\n",
      "11\n",
      "[25034, 278, 10541, 29901, 1724, 338, 278, 7483, 310, 3444, 29973]\n",
      "Position 0: Prompt Token 'Complete' - Top K Tokens: ['▁terminated', '▁triggered', 'Completed', '▁completion', 'noreferrer']\n",
      "Position 1: Prompt Token 'the' - Top K Tokens: ['▁най', '▁się', '▁zur', '<0xE3>', '▁dalla']\n",
      "Position 2: Prompt Token 'sentence' - Top K Tokens: ['▁phrase', '▁phr', '▁term', '▁description', '▁words']\n",
      "Position 3: Prompt Token ':' - Top K Tokens: ['▁\":', ')--', '▁Selon', 'zec', '▁ayant']\n",
      "Position 4: Prompt Token 'What' - Top K Tokens: ['▁Would', '▁Usually', '▁Could', '▁Has', '▁Cs']\n",
      "Position 5: Prompt Token 'is' - Top K Tokens: ['irable', '▁із', '▁sust', '▁Isn', '▁proposition']\n",
      "Position 6: Prompt Token 'the' - Top K Tokens: ['：', '▁The', '▁This', '▁het', 'The']\n",
      "Position 7: Prompt Token 'capital' - Top K Tokens: ['▁Andreas', '▁Town', '▁city', '▁island', '▁Virgin']\n",
      "Position 8: Prompt Token 'of' - Top K Tokens: ['icity', 'ive', 'ine', 'antic', 'та']\n",
      "Position 9: Prompt Token 'France' - Top K Tokens: ['emetery', '▁Liberal', 'zef', '▁Originals', '▁Samuel']\n",
      "Position 10: Prompt Token '?' - Top K Tokens: [';', ');', ';\\r', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# We know that the average KL divergence should be like 40ish for a random cold start prompt\n",
    "# The KL divergence isn't used during optimization because we don't have access to the actual prompt during optimization\n",
    "# We can use this as a first step to make sure we arent completely off. We can cheat and take the length of the golden response to initalize our cold start\n",
    "def get_message_format(prompt, response_document, system_prompt=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"):\n",
    "    return [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response_document}]\n",
    "\n",
    "def format_conversation_with_template(tokenizer, messages, include_reply=True):\n",
    "    \"\"\"\n",
    "    Apply the chat template to format the conversation.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer object.\n",
    "        messages (list): List of conversation messages.\n",
    "        include_reply (bool): Whether to include the assistant's reply.\n",
    "\n",
    "    Returns:\n",
    "        input_ids (torch.Tensor): Tokenized and encoded input IDs for the formatted conversation.\n",
    "    \"\"\"\n",
    "    if include_reply:\n",
    "        formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    else:\n",
    "        formatted_text = tokenizer.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=False)\n",
    "    input_ids = tokenizer.encode(formatted_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def compute_loss_and_probability(model, input_ids, prompt_length):\n",
    "    \"\"\"\n",
    "    Compute the loss and log probability of the assistant's reply.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained language model.\n",
    "        input_ids (torch.Tensor): Encoded input IDs for the conversation.\n",
    "        prompt_length (int): Length of the prompt before the assistant's reply.\n",
    "\n",
    "    Returns:\n",
    "        total_loss (torch.Tensor): Total loss over the assistant's reply.\n",
    "        total_log_prob (float): Log probability of the assistant's reply.\n",
    "        total_prob (float): Conditional probability of the assistant's reply.\n",
    "    \"\"\"\n",
    "    outputs = model(inputs_embeds=model.get_input_embeddings()(input_ids))\n",
    "    logits = outputs.logits[:, :-1, :].contiguous()\n",
    "    target_ids = input_ids[:, 1:].contiguous()\n",
    "\n",
    "    continuation_start = prompt_length\n",
    "    continuation_end = target_ids.shape[1]\n",
    "\n",
    "    if continuation_start >= continuation_end:\n",
    "        return 0.0, 0.0, []\n",
    "\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    target_ids_flat = target_ids.view(-1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    loss_tensor = loss_fct(logits_flat, target_ids_flat)\n",
    "    continuation_loss = loss_tensor[continuation_start - 1:]\n",
    "    total_loss = continuation_loss.sum()\n",
    "    total_log_prob = -total_loss.item()\n",
    "    total_prob = torch.exp(torch.tensor(total_log_prob)).item()\n",
    "\n",
    "    return total_loss, total_log_prob, total_prob\n",
    "\n",
    "\n",
    "def compute_gradients(model, input_ids):\n",
    "    \"\"\"\n",
    "    Compute gradients of the loss with respect to input embeddings.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained language model.\n",
    "        input_ids (torch.Tensor): Encoded input IDs.\n",
    "\n",
    "    Returns:\n",
    "        inputs_embeds (torch.Tensor): Input embeddings with gradients enabled.\n",
    "        input_embedding_grads (torch.Tensor): Gradients of input embeddings.\n",
    "    \"\"\"\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    inputs_embeds = embedding_layer(input_ids)\n",
    "    inputs_embeds.requires_grad_(True)\n",
    "    inputs_embeds.retain_grad()\n",
    "\n",
    "    outputs = model(inputs_embeds=inputs_embeds)\n",
    "    logits = outputs.logits[:, :-1, :].contiguous()\n",
    "    target_ids = input_ids[:, 1:].contiguous()\n",
    "\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    target_ids_flat = target_ids.view(-1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    loss = loss_fct(logits_flat, target_ids_flat)\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    return inputs_embeds, inputs_embeds.grad.detach().squeeze(0)\n",
    "\n",
    "def compute_gradients_list(model, input_ids_list):\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    inputs_embeds_list = [embedding_layer(input_ids) for input_ids in input_ids_list]\n",
    "    for inputs_embeds in inputs_embeds_list:\n",
    "        inputs_embeds.requires_grad_(True)\n",
    "        inputs_embeds.retain_grad()\n",
    "\n",
    "    for inputs_embeds, input_ids in zip(inputs_embeds_list, input_ids_list):\n",
    "        outputs = model(inputs_embeds=inputs_embeds)\n",
    "        logits = outputs.logits[:, :-1, :].contiguous()\n",
    "        # print(\"inputs_embeds.shape inside compute_gradients_list (expected [1, seq_len, hidden_size])\", inputs_embeds.shape) # Shape: [1, seq_len, hidden_size]\n",
    "\n",
    "        target_ids = input_ids[:, 1:].contiguous()\n",
    "        # print(\"target_ids.shape inside compute_gradients_list (expected [1, seq_len - 1, hidden_size])\", target_ids.shape) # Shape: [1, seq_len - 1, hidden_size]\n",
    "\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        # print(\"logits.shape inside compute_gradients_list (expected [1, seq_len - 1, vocab_size])\", logits.shape) # Shape: [1, seq_len - 1, vocab_size]\n",
    "        # print(\"logits_flat.shape inside compute_gradients_list (expected [seq_len - 1 * vocab_size])\", logits_flat.shape) # Shape: [seq_len - 1 * vocab_size]\n",
    "        target_ids_flat = target_ids.view(-1)\n",
    "        # print(\"target_ids_flat.shape inside compute_gradients_list (expected [seq_len - 1 * hidden_size])\", target_ids_flat.shape) # Shape: [seq_len - 1 * hidden_size]\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "        loss = loss_fct(logits_flat, target_ids_flat)\n",
    "        loss.backward()\n",
    "\n",
    "    return [inputs_embeds.grad.detach().squeeze(0) for inputs_embeds in inputs_embeds_list] # Shape: [len(input_ids_list), seq_len, hidden_size]\n",
    "\n",
    "    \n",
    "\n",
    "def compute_top_k_tokens_per_position(embedding_layer, input_embedding_grads_list, tokenizer, top_k=5):\n",
    "    \"\"\"\n",
    "    Identify the top K tokens per position based on gradient dot products.\n",
    "\n",
    "    Args:\n",
    "        embedding_layer: Input embedding layer of the model.\n",
    "        input_embedding_grads_list (list[torch.Tensor]): List of gradients of input embeddings for each document.\n",
    "        tokenizer: The tokenizer object.\n",
    "        top_k (int): Number of top tokens to identify.\n",
    "\n",
    "    Returns:\n",
    "        top_k_tokens_per_position (list): List of top K tokens for each position.\n",
    "    \"\"\"\n",
    "    # We want to sum over the gradients for each document at each position\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    embedding_weights = embedding_layer.weight.data\n",
    "    top_k_tokens_per_position = []\n",
    "    top_k_indices_per_position = []\n",
    "    # What shape is input_embedding_grads_list?\n",
    "    # Shape: [bsz, seq_len, hidden_size]\n",
    "    # print(\"len(input_embedding_grads_list)\", len(input_embedding_grads_list))\n",
    "    # print(\"input_embedding_grads_list[0].shape\", input_embedding_grads_list[0].shape)\n",
    "    # Now we want to sum over the gradients for each document at each position\n",
    "    # Shape: sum([bsz, seq_len, hidden_size]) -> [seq_len, hidden_size] -> [seq_len, vocab_size]\n",
    "\n",
    "    summed_gradients = torch.sum(input_embedding_grads_list, dim=0)\n",
    "    # print(\"summed_gradients.shape\", summed_gradients.shape) # weights are [vocab_size, hidden_size], gradients are [seq_len, hidden_size]\n",
    "    dot_products = torch.matmul(summed_gradients, embedding_weights.T) # Shape: [seq_len, vocab_size]\n",
    "    # print(\"dot_products.shape\", dot_products.shape)\n",
    "    _, topk_indices = torch.topk(dot_products, top_k)\n",
    "    # print(\"topk_indices.shape\", topk_indices.shape)\n",
    "    topk_indices = topk_indices.tolist()\n",
    "    topk_tokens = [tokenizer.convert_ids_to_tokens(idxs) for idxs in topk_indices]\n",
    "    # print(\"topk_tokens\", topk_tokens)\n",
    "\n",
    "    return topk_tokens, topk_indices\n",
    "\n",
    "\n",
    "def compute_conditional_probability_and_gradients(model, tokenizer, messages_list, top_k=5):\n",
    "    \"\"\"\n",
    "    Compute the conditional probability of the assistant's reply and identify top K tokens.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained language model.\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        messages_list (list[list]): List of conversation messages for each document\n",
    "        top_k (int): Number of top tokens per position.\n",
    "\n",
    "    Returns:\n",
    "        total_prob (float): Total conditional probability of the assistant's reply.\n",
    "        total_log_prob (float): Log probability of the reply.\n",
    "        top_k_tokens_per_position (list): List of top K tokens for each position.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    # Step 1: Format conversation and compute prompt lengths\n",
    "    input_ids_list = [format_conversation_with_template(tokenizer, messages, include_reply=True).to(device) for messages in messages_list] # Shape: [1, seq_len]\n",
    "    prompt_input_ids_list = [format_conversation_with_template(tokenizer, messages, include_reply=False).to(device) for messages in messages_list] # Shape: [1, prompt_len]\n",
    "    # print(len(prompt_input_ids_list))\n",
    "    # print(prompt_input_ids_list[0])\n",
    "    prompt_lengths = [len(prompt_input_ids[0]) for prompt_input_ids in prompt_input_ids_list]\n",
    "\n",
    "    # Step 2: Compute loss and probabilities\n",
    "    # Shapes: [len(messages_list)]\n",
    "    total_loss_list, total_log_prob_list, total_prob_list = zip(*[compute_loss_and_probability(model, input_ids, prompt_length) for input_ids, prompt_length in zip(input_ids_list, prompt_lengths)])\n",
    "    # print(\"total_loss_list.shape\", total_loss_list)\n",
    "    # print(\"len(total_loss_list)\", len(total_loss_list))\n",
    "    # print(\"total_log_prob_list.shape\", total_log_prob_list)\n",
    "    # print(\"len(total_log_prob_list)\", len(total_log_prob_list))\n",
    "    # print(\"total_prob_list.shape\", total_prob_list)\n",
    "    # print(\"len(total_prob_list)\", len(total_prob_list))\n",
    "    total_loss = sum(total_loss_list) # Shape: 1\n",
    "    total_log_prob = sum(total_log_prob_list) # Shape: 1\n",
    "    total_prob = sum(total_prob_list) # Shape: 1\n",
    "\n",
    "    if total_loss == 0.0:\n",
    "        print(\"WARNING: Assistant's reply is empty\")\n",
    "        return total_prob, total_log_prob, [], []\n",
    "\n",
    "    # Step 3: Find the subarray index of the prompt in the tokenized messages and mask to only consider the gradient of the prompt tokens\n",
    "    prompt_tokens = tokenizer.encode(messages_list[0][-2]['content'], add_special_tokens=False)\n",
    "    tokenized_messages = input_ids_list[0].tolist()\n",
    "    subarray_index = find_subarray_index(tokenized_messages[0], prompt_tokens)\n",
    "    if subarray_index == -1:\n",
    "        print(\"WARNING: Prompt not found in tokenized messages\")\n",
    "        print(\"prompt_tokens\", prompt_tokens)\n",
    "        print(\"prompt token decoded\", tokenizer.decode(prompt_tokens))\n",
    "        print(\"tokenized_messages\", tokenized_messages[0])\n",
    "        print(\"tokenized_messages decoded\", tokenizer.decode(tokenized_messages[0]))\n",
    "        raise ValueError(\"Prompt not found in tokenized messages\")\n",
    "\n",
    "    # Step 4: Compute gradients\n",
    "    input_embedding_grads_list = compute_gradients_list(model, input_ids_list)\n",
    "    input_embedding_grads_list = list(map(lambda x: x[subarray_index:subarray_index + len(prompt_tokens)], input_embedding_grads_list))\n",
    "    input_embedding_grads_tensor = torch.stack(input_embedding_grads_list)\n",
    "\n",
    "    # We should be passing only the prompt tokens to compute_top_k_tokens_per_position\n",
    "    # Step 5: Compute top K tokens per position\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    top_k_tokens_per_position, top_k_indices_per_position = compute_top_k_tokens_per_position(embedding_layer, input_embedding_grads_tensor, tokenizer, top_k)\n",
    "\n",
    "    # top_k_tokens_per_position = top_k_tokens_per_position[subarray_index:subarray_index + len(prompt_tokens)]\n",
    "    # top_k_indices_per_position = top_k_indices_per_position[subarray_index:subarray_index + len(prompt_tokens)]\n",
    "\n",
    "    return total_prob, total_log_prob, top_k_tokens_per_position, top_k_indices_per_position\n",
    "\n",
    "def compute_loss(model, tokenizer, prompt, documents):\n",
    "    # Loss is the average NLL over all documents given a prompt\n",
    "    total_loss = 0.0\n",
    "    for document in documents:\n",
    "        messages = get_message_format(prompt, document)\n",
    "        input_ids = format_conversation_with_template(tokenizer, messages, include_reply=True).to(model.device)\n",
    "        prompt_length = len(format_conversation_with_template(tokenizer, messages, include_reply=False)[0])\n",
    "        # print(\"prompt_length\", prompt_length)\n",
    "        _, loss, _ = compute_loss_and_probability(model, input_ids, prompt_length)\n",
    "        total_loss += loss\n",
    "    return total_loss / len(documents)\n",
    "\n",
    "\n",
    "# Define conversation history\n",
    "prompt = \"Complete the sentence: What is the capital of France?\"\n",
    "documents = [\"Paris is the capital of France\", \"The capital of France is Paris\", \"Paris.\"]\n",
    "prompt_messages = [get_message_format(prompt, document) for document in documents]\n",
    "\n",
    "prob, log_prob, top_k_tokens_per_position, top_k_indices_per_position = compute_conditional_probability_and_gradients(\n",
    "    model, tokenizer, prompt_messages, top_k=5)\n",
    "print(len(top_k_tokens_per_position))\n",
    "\n",
    "print(f\"Conditional Probability: {prob}\")\n",
    "print(f\"Log Probability: {log_prob}\")\n",
    "print(\"Top K Tokens per Position in the Prompt with Greatest Gradients:\")\n",
    "tokenized_prompt = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "print(len(tokenized_prompt))\n",
    "print(tokenized_prompt)\n",
    "# formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "#     else:\n",
    "#         formatted_text = tokenizer.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=False)\n",
    "#     print(formatted_text)\n",
    "#     input_ids = tokenizer.encode(formatted_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "# print(len(tokenized_messages))\n",
    "# print(tokenized_messages)\n",
    "# print(\"subarray index\", find_subarray_index(tokenized_messages, tokenized_prompt))\n",
    "for idx, tokens in enumerate(top_k_tokens_per_position):\n",
    "    token_text = tokenizer.decode([tokenized_prompt[idx]]) if idx < len(tokenized_prompt) else ''\n",
    "    print(f\"Position {idx}: Prompt Token '{token_text}' - Top K Tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_divergence(model, tokenizer, reference_prompt, candidate_prompt, documents):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence between the conditional distributions over the continuation tokens\n",
    "    given by the model when conditioned on the reference prompt and the candidate prompt.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained language model (AutoModelForCausalLM).\n",
    "        tokenizer: Corresponding tokenizer (AutoTokenizer).\n",
    "        reference_messages (list): List of messages for the reference prompt and continuation.\n",
    "        candidate_messages (list): List of messages for the candidate prompt and continuation.\n",
    "        documents (list of str): List of documents to compute the loss over.\n",
    "\n",
    "    Returns:\n",
    "        total_kl_divergence (float): The total KL divergence summed over the continuation tokens.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    reference_messages_list = [get_message_format(reference_prompt, document) for document in documents]\n",
    "    candidate_messages_list = [get_message_format(candidate_prompt, document) for document in documents]\n",
    "\n",
    "    # Prepare inputs for the reference prompt\n",
    "    # print(documents)\n",
    "    # print(reference_messages_list[0])\n",
    "    # print(tokenizer.apply_chat_template(reference_messages_list[0], tokenize=False, add_generation_prompt=False))\n",
    "    ref_formatted_text_list = [tokenizer.apply_chat_template(reference_messages, tokenize=False, add_generation_prompt=False) for reference_messages in reference_messages_list]\n",
    "    ref_input_ids_list = [tokenizer.encode(ref_formatted_text, add_special_tokens=False, return_tensors=\"pt\").to(device) for ref_formatted_text in ref_formatted_text_list]\n",
    "\n",
    "    # Prepare inputs for the candidate prompt\n",
    "    cand_formatted_text_list = [tokenizer.apply_chat_template(candidate_messages, tokenize=False, add_generation_prompt=False) for candidate_messages in candidate_messages_list]\n",
    "    cand_input_ids_list = [tokenizer.encode(cand_formatted_text, add_special_tokens=False, return_tensors=\"pt\").to(device) for cand_formatted_text in cand_formatted_text_list]\n",
    "\n",
    "    # Determine the length of the continuation\n",
    "    # Assume that the last message in the messages list is the assistant's reply (continuation)\n",
    "    # continuation_message = reference_messages[-1][\"content\"]  # Should be the same in both messages\n",
    "    # continuation_input_ids = tokenizer.encode(continuation_message, add_special_tokens=False, return_tensors=\"pt\")[0].to(device)\n",
    "    # Continuation lengths are the lengths of the documents\n",
    "    continuation_lengths = [len(tokenizer.encode(document, add_special_tokens=False)) for document in documents]\n",
    "\n",
    "    # Get model outputs for reference prompt\n",
    "    with torch.no_grad():\n",
    "        ref_outputs_list = [model(ref_input_ids) for ref_input_ids in ref_input_ids_list]\n",
    "        # ref_logits = ref_outputs.logits  # Shape: [1, seq_len, vocab_size]\n",
    "        ref_logits_list = [ref_outputs.logits for ref_outputs in ref_outputs_list] # Shape: [1, seq_len, vocab_size, len(documents)]\n",
    "\n",
    "\n",
    "    # Get model outputs for candidate prompt\n",
    "    with torch.no_grad():\n",
    "        cand_outputs_list = [model(cand_input_ids) for cand_input_ids in cand_input_ids_list]\n",
    "        cand_logits_list = [cand_outputs.logits for cand_outputs in cand_outputs_list] # Shape: [1, seq_len, vocab_size, len(documents)]\n",
    "\n",
    "    # Determine the starting index of the continuation in logits\n",
    "    ref_continuation_start_list = [ref_logits.shape[1] - continuation_length for ref_logits, continuation_length in zip(ref_logits_list, continuation_lengths)]\n",
    "    cand_continuation_start_list = [cand_logits.shape[1] - continuation_length for cand_logits, continuation_length in zip(cand_logits_list, continuation_lengths)]\n",
    "\n",
    "    # Extract logits for the continuation tokens\n",
    "    ref_continuation_logits_list = [ref_logits[:, ref_continuation_start - 1:-1, :] for ref_logits, ref_continuation_start in zip(ref_logits_list, ref_continuation_start_list)] # Align logits with targets\n",
    "    cand_continuation_logits_list = [cand_logits[:, cand_continuation_start - 1:-1, :] for cand_logits, cand_continuation_start in zip(cand_logits_list, cand_continuation_start_list)]\n",
    "\n",
    "    # Compute probabilities (softmax) for both reference and candidate prompts\n",
    "    ref_log_probs_list = [F.log_softmax(ref_continuation_logits, dim=-1) for ref_continuation_logits in ref_continuation_logits_list] # Shape: [1, continuation_length, vocab_size, len(documents)]\n",
    "    cand_log_probs_list = [F.log_softmax(cand_continuation_logits, dim=-1) for cand_continuation_logits in cand_continuation_logits_list]\n",
    "\n",
    "    # Compute KL divergence at each position\n",
    "    kl_divergence_list = [F.kl_div(ref_log_probs, cand_log_probs, reduction='none', log_target=True) for ref_log_probs, cand_log_probs in zip(ref_log_probs_list, cand_log_probs_list)]\n",
    "    kl_divergences = [kl_divergence.sum(dim=-1) for kl_divergence in kl_divergence_list]  # Sum over vocabulary\n",
    "\n",
    "    # Sum over all positions\n",
    "    average_kl_divergence = sum([kl_divergence.sum().item() for kl_divergence in kl_divergences]) / len(kl_divergences)\n",
    "\n",
    "    return average_kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def test_kl_divergence_multiple_documents(model, tokenizer, question, documents):\n",
    "    tokenized_reference_prompt = tokenizer.encode(question, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    len_reference_prompt = len(tokenized_reference_prompt[0])\n",
    "    current_prompt_tokenized = torch.tensor([random.randint(0, tokenizer.vocab_size - 1) for _ in range(len_reference_prompt)])\n",
    "    current_prompt = tokenizer.decode(current_prompt_tokenized)\n",
    "    kl_divergences = []\n",
    "    random_replacement_order = random.sample(range(len_reference_prompt), len_reference_prompt)\n",
    "    for i in random_replacement_order:\n",
    "        kl_divergences.append(compute_kl_divergence(model, tokenizer, question, current_prompt, documents))\n",
    "        # Replace the i-th token in our randomly selected sequence with \n",
    "        current_prompt_tokenized = torch.cat([current_prompt_tokenized[:i], tokenized_reference_prompt[0][i].unsqueeze(0), current_prompt_tokenized[i+1:]])\n",
    "        current_prompt = tokenizer.decode(current_prompt_tokenized)\n",
    "    kl_divergences.append(compute_kl_divergence(model, tokenizer, question, current_prompt, documents))\n",
    "    assert kl_divergences[-1] == 0, \"KL divergence should be 0 when the current prompt is the same as the reference prompt\"\n",
    "    return kl_divergences\n",
    "\n",
    "def get_all_kl_divergences_multiple_docs(model, tokenizer, question, documents):\n",
    "    return test_kl_divergence_multiple_documents(model, tokenizer, question, documents)\n",
    "\n",
    "def plot_kl_divergence_multiple_docs(model, tokenizer, kl_divergences, ax=None, label=None, show=True):\n",
    "    \"\"\"Plot KL divergence for multiple question-documents pairs\"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    # we dont get error bars or means for multiple docs\n",
    "    kl_array = np.array(kl_divergences)\n",
    "    ax.plot(kl_array, label=label)\n",
    "\n",
    "    if show:\n",
    "        if label:\n",
    "            ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_multiple_kl_divergences_multiple_docs(model, tokenizer, question_docs_dict):\n",
    "    \"\"\"Plot KL divergences for multiple question-documents pairs\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for question, documents in question_docs_dict.items():\n",
    "        plot_kl_divergence_multiple_docs(model, tokenizer, get_all_kl_divergences_multiple_docs(model, tokenizer, question, documents), ax=ax, label=question, show=False)\n",
    "    \n",
    "    # ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Example usage:\n",
    "# Single question-documents pair:\n",
    "# plot_kl_divergence(model, tokenizer, question, documents)\n",
    "\n",
    "# Multiple questions:\n",
    "# questions_dict = {\"Why is kobe beef expensive?\": docs1, \"Another question\": docs2}\n",
    "# plot_multiple_kl_divergences(model, tokenizer, questions_dict)\n",
    "# pick 10 questions from final_results\n",
    "questions_dict = {question: final_results[question] for question in random.sample(list(final_results.keys()), 3)}\n",
    "plot_multiple_kl_divergences_multiple_docs(model, tokenizer, questions_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 10\n",
      "Epoch 20\n",
      "Epoch 30\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def greedy_coordinate_gradient(model, tokenizer, reference_prompt, documents, top_k=5, epochs=200):\n",
    "    '''\n",
    "    Input:\n",
    "    - Initial prompt X[1:n]\n",
    "    - Loss function L\n",
    "\n",
    "    Output:\n",
    "        - Optimized prompt X[1:n]\n",
    "\n",
    "    Procedure:\n",
    "    1. Repeat for T epochs:\n",
    "        a. For each token position i in {1, ..., n}:\n",
    "            i. Identify promising token substitutions:\n",
    "                - Compute Xi = TopK(-Gradient of L with respect to the i-th token)\n",
    "\n",
    "            ii. For each candidate token j in Xi:\n",
    "                - Create a copy of the current prompt, X[j]_1:n = X_1:n\n",
    "                - Replace the i-th token with candidate j: x[i]_j = Random choice from Xi\n",
    "\n",
    "            iii. Determine the best replacement:\n",
    "                - Select j* = argmin_j L(X[j]_1:n)\n",
    "\n",
    "            iv. Update the i-th token of the prompt:\n",
    "                - X_1:n = X[j*]_1:n\n",
    "    '''\n",
    "    # NOTE: This is kinda cheating at least knowing the length of the reference prompt\n",
    "    tokenized_reference_prompt = tokenizer.encode(reference_prompt, add_special_tokens=False)\n",
    "    len_reference_prompt = len(tokenized_reference_prompt)\n",
    "    # Initialize the current prompt to be random tokens but the same length as the reference prompt\n",
    "    initial_prompt_tokenized = torch.tensor([random.randint(0, tokenizer.vocab_size - 1) for _ in range(len_reference_prompt)])\n",
    "    current_prompt = tokenizer.decode(initial_prompt_tokenized)\n",
    "    \n",
    "    kl_divergences = []\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}\")\n",
    "        # Need to benchmark current prompt to make sure KL divergence matches the rough pace from the paper\n",
    "        kl_divergences.append(compute_kl_divergence(model, tokenizer, reference_prompt, current_prompt, documents))\n",
    "        candidate_prompts = []\n",
    "        \n",
    "        prompt_messages_list = [get_message_format(current_prompt, document) for document in documents]\n",
    "        # Need to change this to use a continuion and then a batch of continuations\n",
    "        prob, log_prob, top_k_tokens_per_position, top_k_indices_per_position = compute_conditional_probability_and_gradients(\n",
    "            model, tokenizer, prompt_messages_list, top_k=top_k)\n",
    "        # print(len(top_k_tokens_per_position))\n",
    "        # print(top_k_tokens_per_position[0])\n",
    "        current_prompt_tokenized = tokenizer.encode(current_prompt, add_special_tokens=False, return_tensors=\"pt\").squeeze(0)\n",
    "        for i, (tokens, indices) in enumerate(zip(top_k_tokens_per_position, top_k_indices_per_position)):\n",
    "            # assumption that the tokenization wont change the length of the prompt\n",
    "            replacement_token = random.choice(indices)\n",
    "            candidate_prompts.append(torch.cat([current_prompt_tokenized[:i], torch.tensor([replacement_token]), current_prompt_tokenized[i+1:]]))\n",
    "\n",
    "        # iii. Determine the best replacement:\n",
    "        #     - Select j* = argmin_j L(X[j]_1:n)\n",
    "        # Also need batching here\n",
    "        candidate_prompts = [tokenizer.decode(candidate_prompt) for candidate_prompt in candidate_prompts]\n",
    "        # NOTE NOTE NOTE NOTE NOTE: There is a negative sign here and I dont know why it makes it better\n",
    "        best_prompt = min(candidate_prompts, key=lambda x: -compute_loss(model, tokenizer, x, documents))\n",
    "\n",
    "        current_prompt = best_prompt\n",
    "    return current_prompt, kl_divergences\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "question = \"What type of soil is suitable for cactus?\"\n",
    "documents = final_results[question]\n",
    "optimized_prompt, kl_divergences = greedy_coordinate_gradient(model, tokenizer, question, documents, top_k=3, epochs=500)\n",
    "print(optimized_prompt)\n",
    "plot_kl_divergence_multiple_docs(model, tokenizer, kl_divergences, label=question, show=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
