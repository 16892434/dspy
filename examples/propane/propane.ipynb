{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import tiktoken\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Goal is to take a sequence of tokens, and return the topk tokens at position i\n",
    "# Such that the tokens maximize the negative gradient of the loss function across the entire sequence\n",
    "\n",
    "# Initial sequence\n",
    "\n",
    "def setup_model_and_tokenizer():\n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "    chat_template = \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\\\'s questions.' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 %}{{ system_message }}{% endif %}{% if message['role'] == 'user' %}{{ ' USER: ' + message['content'].strip() }}{% elif message['role'] == 'assistant' %}{{ ' ASSISTANT: ' + message['content'].strip() + eos_token }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ' ASSISTANT:' }}{% endif %}\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, chat_template=chat_template)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        \n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2c9ea890be4eb0b56be27b2c76398b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878b30bc880041338fd195e8e0651a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dspy-propane/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/dspy-propane/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/dspy-propane/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/dspy-propane/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "model, tokenizer = setup_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_gradient_tokens(model, tokenizer, sequence, position, top_k=5):\n",
    "#     # Tokenize the input sequence\n",
    "#     messages = [{\"role\": \"user\", \"content\": sequence}]\n",
    "#     inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "#     input_ids = inputs.to(model.device)\n",
    "    \n",
    "#     # Create a copy of input_ids that requires gradient\n",
    "#     input_embeds = model.get_input_embeddings()(input_ids)\n",
    "#     input_embeds.requires_grad_(True)\n",
    "#     input_embeds.retain_grad()\n",
    "    \n",
    "#     # Forward pass\n",
    "#     outputs = model(inputs_embeds=input_embeds)\n",
    "#     logits = outputs.logits\n",
    "    \n",
    "#     # Calculate loss (we'll use the current token prediction loss)\n",
    "#     # Don't shift the logits/labels since we want current token prediction\n",
    "#     loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "#                           input_ids.view(-1))\n",
    "    \n",
    "#     # Calculate gradients\n",
    "#     loss.backward()\n",
    "    \n",
    "#     # Get gradients at the specified position\n",
    "#     position_gradients = input_embeds.grad[0, position]\n",
    "    \n",
    "#     # Get the token embeddings\n",
    "#     token_embeddings = model.get_input_embeddings().weight\n",
    "    \n",
    "#     # Calculate similarity between position gradients and all token embeddings\n",
    "#     similarities = torch.matmul(token_embeddings, position_gradients)\n",
    "    \n",
    "#     # Get top-k tokens that maximize the negative gradient\n",
    "#     top_values, top_indices = torch.topk(-similarities, k=top_k)\n",
    "    \n",
    "#     # Convert token ids to tokens\n",
    "#     top_tokens = [tokenizer.decode([idx.item()]) for idx in top_indices]\n",
    "    \n",
    "#     return top_tokens, list(map(lambda x: -x, top_values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sequence: What is the capital of France?\n",
      "Converting to chatml format\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What is the capital of France? ASSISTANT:\n",
      "\n",
      "Tokenized sequence length: 46\n",
      "Analyzing gradient-based importance for each position...\n",
      "\n",
      "Position 0 (current token: '<s>')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'A', Score: -0.6602\n",
      "  Token: '\n",
      "', Score: -0.2008\n",
      "  Token: 'А', Score: -0.1494\n",
      "  Token: 'A', Score: -0.1008\n",
      "  Token: 'The', Score: -0.0703\n",
      "\n",
      "Position 1 (current token: 'A')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'disag', Score: -0.0797\n",
      "  Token: 'indic', Score: -0.0774\n",
      "  Token: 'compet', Score: -0.0681\n",
      "  Token: 'autonom', Score: -0.0680\n",
      "  Token: 'kilom', Score: -0.0672\n",
      "\n",
      "Position 2 (current token: 'chat')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'eigh', Score: -0.0774\n",
      "  Token: 'боль', Score: -0.0742\n",
      "  Token: 'comprom', Score: -0.0715\n",
      "  Token: 'wob', Score: -0.0713\n",
      "  Token: 'overlap', Score: -0.0712\n",
      "\n",
      "Position 3 (current token: 'between')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: '定', Score: -0.0510\n",
      "  Token: 'р', Score: -0.0499\n",
      "  Token: 'し', Score: -0.0492\n",
      "  Token: 'get', Score: -0.0490\n",
      "  Token: 'U', Score: -0.0489\n",
      "\n",
      "Position 4 (current token: 'a')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'odd', Score: -0.0472\n",
      "  Token: 'cur', Score: -0.0356\n",
      "  Token: 'keeping', Score: -0.0322\n",
      "  Token: 'Dic', Score: -0.0322\n",
      "  Token: 'Curt', Score: -0.0321\n",
      "\n",
      "Position 5 (current token: 'curious')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'Enum', Score: -0.0577\n",
      "  Token: 'javafx', Score: -0.0555\n",
      "  Token: 'GitHub', Score: -0.0549\n",
      "  Token: 'игра', Score: -0.0549\n",
      "  Token: '<?', Score: -0.0547\n",
      "\n",
      "Position 6 (current token: 'user')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'gt', Score: -0.0778\n",
      "  Token: 'ocation', Score: -0.0741\n",
      "  Token: 'zer', Score: -0.0728\n",
      "  Token: 'bit', Score: -0.0726\n",
      "  Token: 'ucker', Score: -0.0723\n",
      "\n",
      "Position 7 (current token: 'and')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'ę', Score: -0.0273\n",
      "  Token: 'ą', Score: -0.0271\n",
      "  Token: 'een', Score: -0.0268\n",
      "  Token: 'a', Score: -0.0266\n",
      "  Token: 'ра', Score: -0.0261\n",
      "\n",
      "Position 8 (current token: 'an')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'lyph', Score: -0.0351\n",
      "  Token: 'artific', Score: -0.0337\n",
      "  Token: 'artifact', Score: -0.0336\n",
      "  Token: '删', Score: -0.0327\n",
      "  Token: 'сут', Score: -0.0326\n",
      "\n",
      "Position 9 (current token: 'artificial')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'ि', Score: -0.0409\n",
      "  Token: 'cted', Score: -0.0396\n",
      "  Token: 'lib', Score: -0.0348\n",
      "  Token: 'ி', Score: -0.0345\n",
      "  Token: 'vt', Score: -0.0340\n",
      "\n",
      "Position 10 (current token: 'intelligence')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'serial', Score: -0.0426\n",
      "  Token: 'sert', Score: -0.0408\n",
      "  Token: 'Sund', Score: -0.0378\n",
      "  Token: 'jax', Score: -0.0371\n",
      "  Token: 'Santa', Score: -0.0367\n",
      "\n",
      "Position 11 (current token: 'assistant')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'ly', Score: -0.1072\n",
      "  Token: '�', Score: -0.1064\n",
      "  Token: '�', Score: -0.1046\n",
      "  Token: '�', Score: -0.1045\n",
      "  Token: '�', Score: -0.1043\n",
      "\n",
      "Position 12 (current token: '.')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'и', Score: -0.0426\n",
      "  Token: 'di', Score: -0.0406\n",
      "  Token: '�', Score: -0.0394\n",
      "  Token: 'не', Score: -0.0365\n",
      "  Token: 'an', Score: -0.0359\n",
      "\n",
      "Position 13 (current token: 'The')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'aby', Score: -0.0527\n",
      "  Token: 'el', Score: -0.0464\n",
      "  Token: 'ny', Score: -0.0462\n",
      "  Token: 'AI', Score: -0.0437\n",
      "  Token: 'Ball', Score: -0.0433\n",
      "\n",
      "Position 14 (current token: 'assistant')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'giv', Score: -0.0571\n",
      "  Token: 'Give', Score: -0.0560\n",
      "  Token: 'glory', Score: -0.0534\n",
      "  Token: 'accum', Score: -0.0532\n",
      "  Token: 'beg', Score: -0.0506\n",
      "\n",
      "Position 15 (current token: 'gives')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'great', Score: -0.0321\n",
      "  Token: 'evil', Score: -0.0292\n",
      "  Token: 'tal', Score: -0.0285\n",
      "  Token: 'bad', Score: -0.0277\n",
      "  Token: 'ree', Score: -0.0276\n",
      "\n",
      "Position 16 (current token: 'helpful')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: '：', Score: -0.0565\n",
      "  Token: 'der', Score: -0.0552\n",
      "  Token: '・', Score: -0.0544\n",
      "  Token: 'в', Score: -0.0544\n",
      "  Token: 'ein', Score: -0.0543\n",
      "\n",
      "Position 17 (current token: ',')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'Mira', Score: -0.0170\n",
      "  Token: 'tta', Score: -0.0167\n",
      "  Token: 'Ş', Score: -0.0165\n",
      "  Token: 'ktur', Score: -0.0162\n",
      "  Token: 'ὶ', Score: -0.0161\n",
      "\n",
      "Position 18 (current token: 'detailed')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: '：', Score: -0.0453\n",
      "  Token: '\").', Score: -0.0447\n",
      "  Token: '(!', Score: -0.0438\n",
      "  Token: 'în', Score: -0.0433\n",
      "  Token: '\"></', Score: -0.0433\n",
      "\n",
      "Position 19 (current token: ',')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: '￼', Score: -0.0292\n",
      "', Score: -0.0291\n",
      "', Score: -0.0282\n",
      "', Score: -0.0279\n",
      "  Token: '[,', Score: -0.0278\n",
      "\n",
      "Position 20 (current token: 'and')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'По', Score: -0.0279\n",
      "  Token: 'ent', Score: -0.0242\n",
      "  Token: 'Pologne', Score: -0.0235\n",
      "  Token: 'по', Score: -0.0229\n",
      "  Token: 'Pr', Score: -0.0227\n",
      "\n",
      "Position 21 (current token: 'pol')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'She', Score: -0.0159\n",
      "  Token: 'Night', Score: -0.0157\n",
      "  Token: 'Ш', Score: -0.0154\n",
      "  Token: 'Hann', Score: -0.0150\n",
      "  Token: '’', Score: -0.0149\n",
      "\n",
      "Position 22 (current token: 'ite')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'with', Score: -0.0112\n",
      "  Token: 'from', Score: -0.0111\n",
      "  Token: 'into', Score: -0.0101\n",
      "  Token: 'on', Score: -0.0096\n",
      "  Token: 'than', Score: -0.0092\n",
      "\n",
      "Position 23 (current token: 'answers')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: '[-', Score: -0.0449\n",
      "  Token: '$[', Score: -0.0441\n",
      "  Token: '\"/', Score: -0.0439\n",
      "  Token: '=(', Score: -0.0425\n",
      "  Token: '(\"/', Score: -0.0424\n",
      "\n",
      "Position 24 (current token: 'to')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: ' ', Score: -0.0561\n",
      "  Token: '$\\{', Score: -0.0551\n",
      "  Token: '$[', Score: -0.0543\n",
      "  Token: '\"[', Score: -0.0527\n",
      "  Token: '\\\"', Score: -0.0526\n",
      "\n",
      "Position 25 (current token: 'the')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'didn', Score: -0.0119\n",
      "  Token: 'won', Score: -0.0084\n",
      "  Token: 'couldn', Score: -0.0069\n",
      "  Token: 'wasn', Score: -0.0067\n",
      "  Token: 'don', Score: -0.0063\n",
      "\n",
      "Position 26 (current token: 'user')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'RIG', Score: -0.0383\n",
      "  Token: 'н', Score: -0.0368\n",
      "  Token: 'u', Score: -0.0366\n",
      "  Token: 'ület', Score: -0.0365\n",
      "  Token: 'punkt', Score: -0.0363\n",
      "\n",
      "Position 27 (current token: ''')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: ''@', Score: -0.0201\n",
      "  Token: 'ˆ', Score: -0.0195\n",
      "  Token: '调', Score: -0.0194\n",
      "  Token: '\\_', Score: -0.0194\n",
      "  Token: '\".$', Score: -0.0192\n",
      "\n",
      "Position 28 (current token: 's')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'dinner', Score: -0.0083\n",
      "  Token: 'an', Score: -0.0080\n",
      "  Token: 'A', Score: -0.0077\n",
      "  Token: 'д', Score: -0.0076\n",
      "  Token: 'Д', Score: -0.0075\n",
      "\n",
      "Position 29 (current token: 'questions')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: ' ', Score: -0.0913\n",
      "  Token: 'были', Score: -0.0899\n",
      "  Token: 'для', Score: -0.0892\n",
      "  Token: ' ', Score: -0.0886\n",
      "  Token: 'het', Score: -0.0886\n",
      "\n",
      "Position 30 (current token: '.')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'NS', Score: -0.0297\n",
      "  Token: 'MM', Score: -0.0272\n",
      "  Token: 'SP', Score: -0.0246\n",
      "  Token: 'LM', Score: -0.0235\n",
      "  Token: 'LT', Score: -0.0227\n",
      "\n",
      "Position 31 (current token: 'US')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'opol', Score: -0.0509\n",
      "  Token: 'esk', Score: -0.0490\n",
      "  Token: 'emon', Score: -0.0478\n",
      "  Token: 'User', Score: -0.0471\n",
      "  Token: 'GB', Score: -0.0453\n",
      "\n",
      "Position 32 (current token: 'ER')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: '’', Score: -0.0053\n",
      "  Token: ''', Score: -0.0002\n",
      "  Token: 'm', Score: -0.0000\n",
      "  Token: '9', Score: -0.0000\n",
      "  Token: 'h', Score: -0.0000\n",
      "\n",
      "Position 33 (current token: ':')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'Jac', Score: -0.0215\n",
      "  Token: 'Burg', Score: -0.0211\n",
      "  Token: 'agine', Score: -0.0205\n",
      "  Token: 'Mey', Score: -0.0201\n",
      "  Token: 'iful', Score: -0.0199\n",
      "\n",
      "Position 34 (current token: 'What')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: '_', Score: -0.0246\n",
      "  Token: '�', Score: -0.0240\n",
      "  Token: ''', Score: -0.0234\n",
      "  Token: '-', Score: -0.0230\n",
      "  Token: '（', Score: -0.0230\n",
      "\n",
      "Position 35 (current token: 'is')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: ' ', Score: -0.0482\n",
      "  Token: '\\}', Score: -0.0473\n",
      "  Token: ':@\"', Score: -0.0468\n",
      "  Token: ''}', Score: -0.0468\n",
      "  Token: '-->', Score: -0.0465\n",
      "\n",
      "Position 36 (current token: 'the')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'central', Score: -0.0363\n",
      "  Token: 'Haupt', Score: -0.0354\n",
      "  Token: 'う', Score: -0.0348\n",
      "  Token: 'primeira', Score: -0.0347\n",
      "  Token: 'Cho', Score: -0.0320\n",
      "\n",
      "Position 37 (current token: 'capital')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: ' ', Score: -0.0382\n",
      "  Token: 'Park', Score: -0.0376\n",
      "  Token: 'Charles', Score: -0.0370\n",
      "  Token: 'Pot', Score: -0.0366\n",
      "  Token: 'Kar', Score: -0.0365\n",
      "\n",
      "Position 38 (current token: 'of')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'streets', Score: -0.0242\n",
      "  Token: 'city', Score: -0.0241\n",
      "  Token: '真', Score: -0.0231\n",
      "  Token: 'reet', Score: -0.0227\n",
      "  Token: 'country', Score: -0.0227\n",
      "\n",
      "Position 39 (current token: 'France')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: ':', Score: -0.0978\n",
      "  Token: '-', Score: -0.0754\n",
      "  Token: '–', Score: -0.0665\n",
      "  Token: '•', Score: -0.0602\n",
      "  Token: ';', Score: -0.0600\n",
      "\n",
      "Position 40 (current token: '?')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'an', Score: -0.1083\n",
      "  Token: 'a', Score: -0.0960\n",
      "  Token: 'ɡ', Score: -0.0811\n",
      "  Token: 'ned', Score: -0.0764\n",
      "  Token: 'á', Score: -0.0725\n",
      "\n",
      "Position 41 (current token: 'A')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'Edd', Score: -0.0256\n",
      "  Token: 'д', Score: -0.0253\n",
      "  Token: 'Ell', Score: -0.0237\n",
      "  Token: 'Ét', Score: -0.0237\n",
      "  Token: 'kk', Score: -0.0233\n",
      "\n",
      "Position 42 (current token: 'SS')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'Исто', Score: -0.0478\n",
      "  Token: 'Dist', Score: -0.0466\n",
      "  Token: 'Imp', Score: -0.0420\n",
      "  Token: 'Hist', Score: -0.0399\n",
      "  Token: 'Import', Score: -0.0397\n",
      "\n",
      "Position 43 (current token: 'IST')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 'ICATION', Score: -0.0207\n",
      "  Token: 'ITY', Score: -0.0200\n",
      "  Token: 'IAL', Score: -0.0195\n",
      "  Token: 'IES', Score: -0.0186\n",
      "  Token: 'MENT', Score: -0.0186\n",
      "\n",
      "Position 44 (current token: 'ANT')\n",
      "Top replacement tokens by gradient magnitude:\n",
      "  Token: 's', Score: -0.0045\n",
      "  Token: 'DS', Score: -0.0045\n",
      "  Token: 'Y', Score: -0.0039\n",
      "  Token: 'VD', Score: -0.0037\n",
      "  Token: 'WN', Score: -0.0035\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sequence = \"What is the capital of France?\"\n",
    "\n",
    "# print(f\"Analyzing sequence: {sequence}\")\n",
    "\n",
    "# print(\"Converting to chatml format\")\n",
    "# messages = [{\"role\": \"user\", \"content\": sequence}]\n",
    "# inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "# # Get tokenized sequence length\n",
    "# tokens = tokenizer.encode(inputs, return_tensors=\"pt\")[0]\n",
    "# seq_length = len(tokens)\n",
    "\n",
    "# print(f\"\\nTokenized sequence length: {seq_length}\")\n",
    "# print(\"Analyzing gradient-based importance for each position...\")\n",
    "# # only change the tokens that are a part of the prompt\n",
    "# prompt_tokens = tokenizer.encode(sequence, return_tensors=\"pt\")[0]\n",
    "# # Analyze each position\n",
    "# # NOTE: The original GCG paper randomly selects between all positions and takes a uniform sample between the top k replacements\n",
    "# # We also need to limit this to only adjusting the task prompt tokens, not the chatml format or eventually the \"generated\" tokens\n",
    "# for i in range(seq_length - 1):\n",
    "#     tokens_at_pos = tokenizer.decode([tokens[i]])\n",
    "#     print(f\"\\nPosition {i} (current token: '{tokens_at_pos}')\")\n",
    "    \n",
    "#     top_tokens, importance_scores = get_gradient_tokens(model, tokenizer, sequence, i)\n",
    "    \n",
    "#     print(\"Top replacement tokens by gradient magnitude:\")\n",
    "#     for token, score in zip(top_tokens, importance_scores):\n",
    "#         print(f\"  Token: '{token}', Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "import dspy\n",
    "import json\n",
    "from dspy.datasets import DataLoader\n",
    "alpaca_dataset = DataLoader().from_huggingface(\"tatsu-lab/alpaca_eval\",  trust_remote_code=True)\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "import concurrent.futures\n",
    "client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "prompts = [x[\"instruction\"] for x in alpaca_dataset[\"eval\"][:100]]\n",
    "\n",
    "\n",
    "# We are going to generate N sample responses for each prompt using gpt-4o-mini\n",
    "def generate_responses(prompt, N, max_workers=10, good_system_prompt=True):\n",
    "    max_workers = min(max_workers, N)\n",
    "    system_prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\" if good_system_prompt else \"Below is an instruction that describes a task. Write a response that makes grammatical sense, but semantically makes no sense.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    responses = []\n",
    "    def generate_response(*args):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=1.5,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        responses = list(executor.map(generate_response, range(N)))\n",
    "    return responses\n",
    "\n",
    "GENERATE_RESULTS = False\n",
    "if GENERATE_RESULTS:\n",
    "    final_results = {}\n",
    "    for prompt in prompts:\n",
    "        final_results[prompt] = generate_responses(prompt, 10)\n",
    "\n",
    "    import json\n",
    "    with open(\"alpaca_results.json\", \"w\") as f:\n",
    "        json.dump(final_results, f)\n",
    "else:\n",
    "    with open(\"alpaca_results.json\", \"r\") as f:\n",
    "        final_results = json.load(f)\n",
    "    with open(\"alpaca_results_bad.json\", \"r\") as f:\n",
    "        final_results_bad = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Results\n",
      "Average Log Probability: 83.14709375\n",
      "Bad Results\n",
      "Average Log Probability: 23.88515625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# def compute_conditional_probability(model, tokenizer, messages):\n",
    "#     \"\"\"\n",
    "#     Compute the conditional probability of the assistant's reply given the conversation history.\n",
    "\n",
    "#     Args:\n",
    "#         model: Pretrained language model (AutoModelForCausalLM).\n",
    "#         tokenizer: Corresponding tokenizer (AutoTokenizer).\n",
    "#         messages (list): List of messages in the conversation, including the assistant's reply as the last message.\n",
    "#             Each message is a dict with keys 'role' and 'content'.\n",
    "\n",
    "#     Returns:\n",
    "#         total_prob (float): The total conditional probability of the assistant's reply.\n",
    "#         total_log_prob (float): The total log probability.\n",
    "#     \"\"\"\n",
    "#     # Apply chat template to the full conversation (including assistant's reply)\n",
    "#     formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "#     input_ids = tokenizer.encode(formatted_text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "#     # Apply chat template to the prompt messages (excluding assistant's reply)\n",
    "#     prompt_messages = messages[:-1]\n",
    "#     formatted_prompt_text = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "#     prompt_input_ids = tokenizer.encode(formatted_prompt_text, add_special_tokens=False, return_tensors=\"pt\")[0].to(model.device)\n",
    "#     prompt_length = len(prompt_input_ids)\n",
    "\n",
    "#     # Get model outputs\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids)\n",
    "#         logits = outputs.logits  # Shape: [batch_size, seq_len, vocab_size]\n",
    "#         # print(\"logits\", logits.shape)\n",
    "\n",
    "#     logits = logits[:, :-1, :]  # Exclude the last token's logits\n",
    "#     logits = logits.squeeze(0)  # Shape: [seq_len - 1, vocab_size]\n",
    "\n",
    "#     # Target tokens are the input_ids shifted to the left\n",
    "#     target_tokens = input_ids.squeeze(0)[1:]  # Exclude the first token\n",
    "\n",
    "#     # Compute probabilities\n",
    "#     probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "#     # Indices for continuation tokens\n",
    "#     continuation_start = prompt_length - 1  # Since target_tokens excludes the first token\n",
    "#     continuation_end = len(target_tokens)\n",
    "\n",
    "#     # Handle the case where the assistant's reply is empty\n",
    "#     if continuation_start >= continuation_end:\n",
    "#         print(\"WARNING: Assistant's reply is empty\")\n",
    "#         total_log_prob = 0.0\n",
    "#         total_prob = 1.0\n",
    "#         return total_prob, total_log_prob\n",
    "\n",
    "#     # Extract log probabilities for continuation tokens\n",
    "#     continuation_probs = probs[continuation_start:continuation_end, :]\n",
    "#     continuation_target_tokens = target_tokens[continuation_start:continuation_end]\n",
    "\n",
    "#     # Get log probabilities for each continuation token\n",
    "#     token_probs = continuation_probs[\n",
    "#         torch.arange(len(continuation_target_tokens)), continuation_target_tokens\n",
    "#     ]\n",
    "    \n",
    "\n",
    "#     # Sum log probabilities\n",
    "#     total_log_prob = token_probs.sum().item()\n",
    "#     total_prob = torch.exp(torch.tensor(total_log_prob))\n",
    "\n",
    "#     return total_prob.item(), total_log_prob\n",
    "\n",
    "# def compute_loss(model, tokenizer, prompt_messages, continuations):\n",
    "#     \"\"\"\n",
    "#     Compute the loss over a list of assistant replies.\n",
    "\n",
    "#     Args:\n",
    "#         model: Pretrained language model (AutoModelForCausalLM).\n",
    "#         tokenizer: Corresponding tokenizer (AutoTokenizer).\n",
    "#         prompt_messages (list): List of messages representing the conversation history (excluding assistant's reply).\n",
    "#         continuations (list of str): List of assistant replies.\n",
    "\n",
    "#     Returns:\n",
    "#         loss (float): The computed loss value.\n",
    "#     \"\"\"\n",
    "#     total_log_prob = 0.0\n",
    "#     n = len(continuations)\n",
    "\n",
    "#     for continuation in continuations:\n",
    "#         # Build the full conversation including the assistant's reply\n",
    "#         messages = prompt_messages + [{'role': 'assistant', 'content': continuation}]\n",
    "#         _, prob = compute_conditional_probability(model, tokenizer, messages)\n",
    "#         log_prob = torch.log(prob)\n",
    "#         total_log_prob += log_prob\n",
    "\n",
    "#     loss = -total_log_prob / n\n",
    "#     return loss\n",
    "\n",
    "# print(\"Good Results\")\n",
    "# log_probs = []\n",
    "# for prompt in final_results:\n",
    "#     for continuation in final_results[prompt]:\n",
    "#         messages = get_message_format(prompt, continuation)\n",
    "#         prob, log_prob = compute_conditional_probability(model, tokenizer, messages)\n",
    "#         # print(f\"Conditional Probability: {prob}\")\n",
    "#         # print(f\"Log Probability: {log_prob}\")\n",
    "#         log_probs.append(log_prob)\n",
    "# print(f\"Average Log Probability: {sum(log_probs) / len(log_probs)}\")\n",
    "# # Assistant's reply\n",
    "# # continuation = ' in a land far, far away'\n",
    "# print(\"Bad Results\")\n",
    "# log_probs = []\n",
    "# for prompt in final_results_bad:\n",
    "#     for continuation in final_results_bad[prompt]:\n",
    "#         messages = get_message_format(prompt, continuation)\n",
    "#         prob, log_prob = compute_conditional_probability(model, tokenizer, messages)\n",
    "#         # print(f\"Conditional Probability: {prob}\")\n",
    "#         # print(f\"Log Probability: {log_prob}\")\n",
    "#         log_probs.append(log_prob)\n",
    "\n",
    "# print(f\"Average Log Probability: {sum(log_probs) / len(log_probs)}\")\n",
    "# # Compute conditional probability\n",
    "# # messages = prompt_messages + [{'role': 'assistant', 'content': continuation}]\n",
    "# # prob, log_prob = compute_conditional_probability(model, tokenizer, messages)\n",
    "# # print(f\"Conditional Probability: {prob}\")\n",
    "# # print(f\"Log Probability: {log_prob}\")\n",
    "\n",
    "# # Compute loss over multiple continuations\n",
    "# # calculate average conditional probability for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subarray_index(main_list, subarray):\n",
    "    \"\"\"\n",
    "    Find the starting index of a subarray in a main list.\n",
    "\n",
    "    Args:\n",
    "        main_list (list): The larger list.\n",
    "        subarray (list): The subarray to search for.\n",
    "\n",
    "    Returns:\n",
    "        int: The starting index of the subarray in the main list, or -1 if not found.\n",
    "    \"\"\"\n",
    "    n, m = len(main_list), len(subarray)\n",
    "    for i in range(n - m + 1):\n",
    "        if main_list[i:i + m] == subarray:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request. USER: Complete the sentence: The quick brown fox ASSISTANT: jumped over the lazy dog</s>\n",
      "\n",
      "9\n",
      "Conditional Probability: 0.0023931225296109915\n",
      "Log Probability: -6.03515625\n",
      "Top K Tokens per Position in the Prompt with Greatest Gradients:\n",
      "9\n",
      "[25034, 278, 10541, 29901, 450, 4996, 17354, 1701, 29916]\n",
      "44\n",
      "[13866, 338, 385, 15278, 393, 16612, 263, 3414, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 3148, 1001, 29901, 25034, 278, 10541, 29901, 450, 4996, 17354, 1701, 29916, 319, 1799, 9047, 13566, 29901, 12500, 287, 975, 278, 17366, 11203, 2]\n",
      "subarray index 23\n",
      "Position 0: Prompt Token 'Complete' - Top K Tokens: ['gradually', 'terminal', 'uffle', 'Here', 'activation']\n",
      "Position 1: Prompt Token 'the' - Top K Tokens: ['）', '-', '/)', '️', 'А']\n",
      "Position 2: Prompt Token 'sentence' - Top K Tokens: ['phrase', 'word', 'words', 'expression', 'phr']\n",
      "Position 3: Prompt Token ':' - Top K Tokens: ['constit', 'deliber', 'rejo', 'qued', 'scattered']\n",
      "Position 4: Prompt Token 'The' - Top K Tokens: ['zA', '={{', '[`', '[{', 'Amts']\n",
      "Position 5: Prompt Token 'quick' - Top K Tokens: ['Async', 'Rica', 'shoulder', 'traduc', 'Golden']\n",
      "Position 6: Prompt Token 'brown' - Top K Tokens: ['Fland', 'mud', 'xyz', 'х', 'hyp']\n",
      "Position 7: Prompt Token 'fo' - Top K Tokens: ['ví', 'sw', 'demon', '函', 'conflic']\n",
      "Position 8: Prompt Token 'x' - Top K Tokens: ['}}}\\\\', '}}}', 'wend', 'kunft', '}))']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# We know that the average KL divergence should be like 40ish for a random cold start prompt\n",
    "# The KL divergence isn't used during optimization because we don't have access to the actual prompt during optimization\n",
    "# We can use this as a first step to make sure we arent completely off. We can cheat and take the length of the golden response to initalize our cold start\n",
    "def get_message_format(prompt, response_document, system_prompt=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"):\n",
    "    return [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response_document}]\n",
    "\n",
    "def format_conversation_with_template(tokenizer, messages, include_reply=True):\n",
    "    \"\"\"\n",
    "    Apply the chat template to format the conversation.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer object.\n",
    "        messages (list): List of conversation messages.\n",
    "        include_reply (bool): Whether to include the assistant's reply.\n",
    "\n",
    "    Returns:\n",
    "        input_ids (torch.Tensor): Tokenized and encoded input IDs for the formatted conversation.\n",
    "    \"\"\"\n",
    "    if include_reply:\n",
    "        formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    else:\n",
    "        formatted_text = tokenizer.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=False)\n",
    "    print(formatted_text)\n",
    "    input_ids = tokenizer.encode(formatted_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def compute_loss_and_probability(model, input_ids, prompt_length):\n",
    "    \"\"\"\n",
    "    Compute the loss and log probability of the assistant's reply.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained language model.\n",
    "        input_ids (torch.Tensor): Encoded input IDs for the conversation.\n",
    "        prompt_length (int): Length of the prompt before the assistant's reply.\n",
    "\n",
    "    Returns:\n",
    "        total_loss (torch.Tensor): Total loss over the assistant's reply.\n",
    "        total_log_prob (float): Log probability of the assistant's reply.\n",
    "        total_prob (float): Conditional probability of the assistant's reply.\n",
    "    \"\"\"\n",
    "    outputs = model(inputs_embeds=model.get_input_embeddings()(input_ids))\n",
    "    logits = outputs.logits[:, :-1, :].contiguous()\n",
    "    target_ids = input_ids[:, 1:].contiguous()\n",
    "\n",
    "    continuation_start = prompt_length\n",
    "    continuation_end = target_ids.shape[1]\n",
    "\n",
    "    if continuation_start >= continuation_end:\n",
    "        return 0.0, 0.0, []\n",
    "\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    target_ids_flat = target_ids.view(-1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    loss_tensor = loss_fct(logits_flat, target_ids_flat)\n",
    "    continuation_loss = loss_tensor[continuation_start - 1:]\n",
    "    total_loss = continuation_loss.sum()\n",
    "    total_log_prob = -total_loss.item()\n",
    "    total_prob = torch.exp(torch.tensor(total_log_prob)).item()\n",
    "\n",
    "    return total_loss, total_log_prob, total_prob\n",
    "\n",
    "\n",
    "def compute_gradients(model, input_ids):\n",
    "    \"\"\"\n",
    "    Compute gradients of the loss with respect to input embeddings.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained language model.\n",
    "        input_ids (torch.Tensor): Encoded input IDs.\n",
    "\n",
    "    Returns:\n",
    "        inputs_embeds (torch.Tensor): Input embeddings with gradients enabled.\n",
    "        input_embedding_grads (torch.Tensor): Gradients of input embeddings.\n",
    "    \"\"\"\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    inputs_embeds = embedding_layer(input_ids)\n",
    "    inputs_embeds.requires_grad_(True)\n",
    "    inputs_embeds.retain_grad()\n",
    "\n",
    "    outputs = model(inputs_embeds=inputs_embeds)\n",
    "    logits = outputs.logits[:, :-1, :].contiguous()\n",
    "    target_ids = input_ids[:, 1:].contiguous()\n",
    "\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    target_ids_flat = target_ids.view(-1)\n",
    "\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    loss = loss_fct(logits_flat, target_ids_flat)\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    return inputs_embeds, inputs_embeds.grad.detach().squeeze(0)\n",
    "\n",
    "\n",
    "def compute_top_k_tokens_per_position(embedding_layer, input_embedding_grads, tokenizer, top_k=5):\n",
    "    \"\"\"\n",
    "    Identify the top K tokens per position based on gradient dot products.\n",
    "\n",
    "    Args:\n",
    "        embedding_layer: Input embedding layer of the model.\n",
    "        input_embedding_grads (torch.Tensor): Gradients of input embeddings.\n",
    "        tokenizer: The tokenizer object.\n",
    "        top_k (int): Number of top tokens to identify.\n",
    "\n",
    "    Returns:\n",
    "        top_k_tokens_per_position (list): List of top K tokens for each position.\n",
    "    \"\"\"\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    embedding_weights = embedding_layer.weight.data\n",
    "    top_k_tokens_per_position = []\n",
    "\n",
    "    for pos in range(input_embedding_grads.size(0)):\n",
    "        grad_at_pos = input_embedding_grads[pos]\n",
    "        dot_products = torch.mv(embedding_weights, grad_at_pos)\n",
    "        _, topk_indices = torch.topk(dot_products.abs(), top_k)\n",
    "        topk_tokens = [tokenizer.decode([idx.item()]) for idx in topk_indices]\n",
    "        top_k_tokens_per_position.append(topk_tokens)\n",
    "\n",
    "    return top_k_tokens_per_position\n",
    "\n",
    "\n",
    "def compute_conditional_probability_and_gradients(model, tokenizer, messages, top_k=5):\n",
    "    \"\"\"\n",
    "    Compute the conditional probability of the assistant's reply and identify top K tokens.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained language model.\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        messages (list): List of conversation messages.\n",
    "        top_k (int): Number of top tokens per position.\n",
    "\n",
    "    Returns:\n",
    "        total_prob (float): Total conditional probability of the assistant's reply.\n",
    "        total_log_prob (float): Log probability of the reply.\n",
    "        top_k_tokens_per_position (list): List of top K tokens for each position.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    # Step 1: Format conversation and compute prompt lengths\n",
    "    input_ids = format_conversation_with_template(tokenizer, messages, include_reply=True).to(device)\n",
    "    prompt_input_ids = format_conversation_with_template(tokenizer, messages[:-1], include_reply=False).to(device)[0]\n",
    "    prompt_length = len(prompt_input_ids)\n",
    "\n",
    "    # Step 2: Compute loss and probabilities\n",
    "    total_loss, total_log_prob, total_prob = compute_loss_and_probability(model, input_ids, prompt_length)\n",
    "\n",
    "    if total_loss == 0.0:\n",
    "        print(\"WARNING: Assistant's reply is empty\")\n",
    "        return total_prob, total_log_prob, []\n",
    "\n",
    "    # Step 3: Compute gradients\n",
    "    inputs_embeds, input_embedding_grads = compute_gradients(model, input_ids)\n",
    "\n",
    "    # Step 4: Compute top K tokens per position\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    top_k_tokens_per_position = compute_top_k_tokens_per_position(embedding_layer, input_embedding_grads, tokenizer, top_k)\n",
    "\n",
    "    # Step 5: Find the subarray index of the prompt in the tokenized messages and mask to only consider the gradient of the prompt tokens\n",
    "    prompt_tokens = tokenizer.encode(messages[-2]['content'], add_special_tokens=False)\n",
    "    subarray_index = find_subarray_index(tokenized_messages, prompt_tokens)\n",
    "    top_k_tokens_per_position = top_k_tokens_per_position[subarray_index:subarray_index + len(prompt_tokens)]\n",
    "\n",
    "    return total_prob, total_log_prob, top_k_tokens_per_position\n",
    "\n",
    "\n",
    "# Define conversation history\n",
    "prompt = \"Complete the sentence: The quick brown fox\"\n",
    "continuation = \"jumped over the lazy dog\"\n",
    "prompt_messages = get_message_format(prompt, continuation)\n",
    "\n",
    "prob, log_prob, top_k_tokens_per_position = compute_conditional_probability_and_gradients(\n",
    "    model, tokenizer, prompt_messages, top_k=5)\n",
    "print(len(top_k_tokens_per_position))\n",
    "\n",
    "print(f\"Conditional Probability: {prob}\")\n",
    "print(f\"Log Probability: {log_prob}\")\n",
    "print(\"Top K Tokens per Position in the Prompt with Greatest Gradients:\")\n",
    "tokenized_prompt = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "print(len(tokenized_prompt))\n",
    "print(tokenized_prompt)\n",
    "# formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "#     else:\n",
    "#         formatted_text = tokenizer.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=False)\n",
    "#     print(formatted_text)\n",
    "#     input_ids = tokenizer.encode(formatted_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "tokenized_messages = tokenizer.encode(tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=False), add_special_tokens=False)\n",
    "print(len(tokenized_messages))\n",
    "print(tokenized_messages)\n",
    "print(\"subarray index\", find_subarray_index(tokenized_messages, tokenized_prompt))\n",
    "for idx, tokens in enumerate(top_k_tokens_per_position):\n",
    "    token_text = tokenizer.decode([tokenized_prompt[idx]]) if idx < len(tokenized_prompt) else ''\n",
    "    print(f\"Position {idx}: Prompt Token '{token_text}' - Top K Tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_divergence(model, tokenizer, reference_prompt, candidate_prompt, documents):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence between the conditional distributions over the continuation tokens\n",
    "    given by the model when conditioned on the reference prompt and the candidate prompt.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained language model (AutoModelForCausalLM).\n",
    "        tokenizer: Corresponding tokenizer (AutoTokenizer).\n",
    "        reference_messages (list): List of messages for the reference prompt and continuation.\n",
    "        candidate_messages (list): List of messages for the candidate prompt and continuation.\n",
    "\n",
    "    Returns:\n",
    "        total_kl_divergence (float): The total KL divergence summed over the continuation tokens.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    reference_messages = get_message_format(reference_prompt, documents[0])\n",
    "    candidate_messages = get_message_format(candidate_prompt, documents[0])\n",
    "\n",
    "    # Prepare inputs for the reference prompt\n",
    "    ref_formatted_text = tokenizer.apply_chat_template(reference_messages, tokenize=False, add_generation_prompt=False)\n",
    "    ref_input_ids = tokenizer.encode(ref_formatted_text, add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Prepare inputs for the candidate prompt\n",
    "    cand_formatted_text = tokenizer.apply_chat_template(candidate_messages, tokenize=False, add_generation_prompt=False)\n",
    "    cand_input_ids = tokenizer.encode(cand_formatted_text, add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Determine the length of the continuation\n",
    "    # Assume that the last message in the messages list is the assistant's reply (continuation)\n",
    "    continuation_message = reference_messages[-1][\"content\"]  # Should be the same in both messages\n",
    "    continuation_input_ids = tokenizer.encode(continuation_message, add_special_tokens=False, return_tensors=\"pt\")[0].to(device)\n",
    "    continuation_length = continuation_input_ids.shape[0]\n",
    "\n",
    "    # Get model outputs for reference prompt\n",
    "    with torch.no_grad():\n",
    "        ref_outputs = model(ref_input_ids)\n",
    "        ref_logits = ref_outputs.logits  # Shape: [1, seq_len, vocab_size]\n",
    "\n",
    "    # Get model outputs for candidate prompt\n",
    "    with torch.no_grad():\n",
    "        cand_outputs = model(cand_input_ids)\n",
    "        cand_logits = cand_outputs.logits  # Shape: [1, seq_len, vocab_size]\n",
    "\n",
    "    # Determine the starting index of the continuation in logits\n",
    "    ref_continuation_start = ref_logits.shape[1] - continuation_length\n",
    "    cand_continuation_start = cand_logits.shape[1] - continuation_length\n",
    "\n",
    "    # Extract logits for the continuation tokens\n",
    "    ref_continuation_logits = ref_logits[:, ref_continuation_start - 1:-1, :]  # Align logits with targets\n",
    "    cand_continuation_logits = cand_logits[:, cand_continuation_start - 1:-1, :]\n",
    "\n",
    "    # Compute probabilities (softmax) for both reference and candidate prompts\n",
    "    ref_log_probs = F.log_softmax(ref_continuation_logits, dim=-1)  # Shape: [1, continuation_length, vocab_size]\n",
    "    cand_log_probs = F.log_softmax(cand_continuation_logits, dim=-1)\n",
    "\n",
    "    # Compute KL divergence at each position\n",
    "    kl_divergence = F.kl_div(ref_log_probs, cand_log_probs, reduction='none', log_target=True)\n",
    "    kl_divergence = kl_divergence.sum(dim=-1)  # Sum over vocabulary\n",
    "\n",
    "    # Sum over all positions\n",
    "    total_kl_divergence = kl_divergence.sum().item()\n",
    "\n",
    "    return total_kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "dual Dakcomes however friend noticed trivialsupportedablingicator arist\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhy is kobe beef so damn expensive?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m documents \u001b[38;5;241m=\u001b[39m final_results[question]\n\u001b[0;32m---> 66\u001b[0m optimized_prompt, kl_divergences \u001b[38;5;241m=\u001b[39m greedy_coordinate_gradient(model, tokenizer, question, documents, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(optimized_prompt)\n\u001b[1;32m     68\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(kl_divergences)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def greedy_coordinate_gradient(model, tokenizer, reference_prompt, documents, top_k=5, epochs=200):\n",
    "    '''\n",
    "    Input:\n",
    "    - Initial prompt X[1:n]\n",
    "    - Loss function L\n",
    "\n",
    "    Output:\n",
    "        - Optimized prompt X[1:n]\n",
    "\n",
    "    Procedure:\n",
    "    1. Repeat for T epochs:\n",
    "        a. For each token position i in {1, ..., n}:\n",
    "            i. Identify promising token substitutions:\n",
    "                - Compute Xi = TopK(-Gradient of L with respect to the i-th token)\n",
    "\n",
    "            ii. For each candidate token j in Xi:\n",
    "                - Create a copy of the current prompt, X[j]_1:n = X_1:n\n",
    "                - Replace the i-th token with candidate j: x[i]_j = Random choice from Xi\n",
    "\n",
    "            iii. Determine the best replacement:\n",
    "                - Select j* = argmin_j L(X[j]_1:n)\n",
    "\n",
    "            iv. Update the i-th token of the prompt:\n",
    "                - X_1:n = X[j*]_1:n\n",
    "    '''\n",
    "    # NOTE: This is kinda cheating at least knowing the length of the reference prompt\n",
    "    tokenized_reference_prompt = tokenizer.encode(reference_prompt, add_special_tokens=False)\n",
    "    len_reference_prompt = len(tokenized_reference_prompt)\n",
    "    # Initialize the current prompt to be random tokens but the same length as the reference prompt\n",
    "    initial_prompt_tokenized = torch.tensor([random.randint(0, tokenizer.vocab_size - 1) for _ in range(len_reference_prompt)])\n",
    "    current_prompt = tokenizer.decode(initial_prompt_tokenized)\n",
    "    \n",
    "    return\n",
    "    kl_divergences = []\n",
    "    for epoch in range(epochs):\n",
    "        # Need to benchmark current prompt to make sure KL divergence matches the rough pace from the paper\n",
    "        kl_divergences.append(compute_kl_divergence(model, tokenizer, reference_prompt, current_prompt, documents))\n",
    "        candidate_prompts = []\n",
    "        \n",
    "        prompt_messages = get_message_format(current_prompt, documents[0])\n",
    "        # Need to change this to use a continuion and then a batch of continuations\n",
    "        prob, log_prob, top_k_tokens_per_position = compute_conditional_probability_and_gradients(\n",
    "            model, tokenizer, prompt_messages, top_k=top_k)\n",
    "        for i, tokens in enumerate(top_k_tokens_per_position):\n",
    "            replacement_token = random.choice(tokens)\n",
    "            candidate_prompts.append(current_prompt[:i] + [replacement_token] + current_prompt[i+1:])\n",
    "\n",
    "        # iii. Determine the best replacement:\n",
    "        #     - Select j* = argmin_j L(X[j]_1:n)\n",
    "        # Also need batching here\n",
    "        best_prompt = min(candidate_prompts, key=lambda x: compute_loss(model, tokenizer, prompt_messages, [x]))\n",
    "\n",
    "        current_prompt = best_prompt\n",
    "    return current_prompt, kl_divergences\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "question = \"Why is kobe beef so damn expensive?\"\n",
    "documents = final_results[question]\n",
    "optimized_prompt, kl_divergences = greedy_coordinate_gradient(model, tokenizer, question, documents, top_k=5, epochs=10)\n",
    "print(optimized_prompt)\n",
    "plt.plot(kl_divergences)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def test_kl_divergence_single_document(model, tokenizer, question, document):\n",
    "    # Instead of actuall performing the search with gradient descent, we will just replace the tokens with the reference tokens one by one to see the KL divergence\n",
    "    tokenized_reference_prompt = tokenizer.encode(question, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    len_reference_prompt = len(tokenized_reference_prompt[0])\n",
    "    current_prompt_tokenized = torch.tensor([random.randint(0, tokenizer.vocab_size - 1) for _ in range(len_reference_prompt)])\n",
    "    current_prompt = tokenizer.decode(current_prompt_tokenized)\n",
    "    kl_divergences = []\n",
    "    random_replacement_order = random.sample(range(len_reference_prompt), len_reference_prompt)\n",
    "    for i in random_replacement_order:\n",
    "        kl_divergences.append(compute_kl_divergence(model, tokenizer, question, current_prompt, document))\n",
    "        # Replace the i-th token in our randomly selected sequence with \n",
    "        current_prompt_tokenized = torch.cat([current_prompt_tokenized[:i], tokenized_reference_prompt[0][i].unsqueeze(0), current_prompt_tokenized[i+1:]])\n",
    "        current_prompt = tokenizer.decode(current_prompt_tokenized)\n",
    "    kl_divergences.append(compute_kl_divergence(model, tokenizer, question, current_prompt, document))\n",
    "    assert kl_divergences[-1] == 0, \"KL divergence should be 0 when the current prompt is the same as the reference prompt\"\n",
    "    return kl_divergences\n",
    "\n",
    "def get_all_kl_divergences(model, tokenizer, question, documents):\n",
    "    kl_divergences = []\n",
    "    for document in documents:\n",
    "        kl_divergences.append(test_kl_divergence_single_document(model, tokenizer, question, document))\n",
    "    return kl_divergences\n",
    "\n",
    "def plot_kl_divergence(model, tokenizer, question, documents, ax=None, label=None, show=True):\n",
    "    \"\"\"Plot KL divergence for a single question-documents pair\"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "        \n",
    "    kl_divergences = get_all_kl_divergences(model, tokenizer, question, documents)\n",
    "    kl_array = np.array(kl_divergences)\n",
    "    mean = np.mean(kl_array, axis=0)\n",
    "    std = np.std(kl_array, axis=0)\n",
    "    print(mean[-1])\n",
    "\n",
    "    x = range(len(mean))\n",
    "    ax.plot(x, mean, label=label or question)\n",
    "    ax.fill_between(x, mean-std, mean+std, alpha=0.3)\n",
    "    \n",
    "    if show:\n",
    "        if label:\n",
    "            ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def plot_multiple_kl_divergences(model, tokenizer, question_docs_dict):\n",
    "    \"\"\"Plot KL divergences for multiple question-documents pairs\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for question, documents in question_docs_dict.items():\n",
    "        plot_kl_divergence(model, tokenizer, question, documents, ax=ax, label=question, show=False)\n",
    "    \n",
    "    # ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Example usage:\n",
    "# Single question-documents pair:\n",
    "# plot_kl_divergence(model, tokenizer, question, documents)\n",
    "\n",
    "# Multiple questions:\n",
    "# questions_dict = {\"Why is kobe beef expensive?\": docs1, \"Another question\": docs2}\n",
    "# plot_multiple_kl_divergences(model, tokenizer, questions_dict)\n",
    "# pick 10 questions from final_results\n",
    "questions_dict = {question: final_results[question] for question in random.sample(list(final_results.keys()), 10)}\n",
    "plot_multiple_kl_divergences(model, tokenizer, questions_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
