import dsp
from dsp.utils import EM

def answer_and_context_match(d, program, train):
  x = dsp.Example(question=d.question, demos=dsp.all_but(train, d))
  x = program(x)
  if not dsp.passage_match(x.context, d.answer): return None
  if not dsp.answer_match(x.answer, d.answer): return None
  return d.copy(**x)

def approximate_answer_match(d, program, train):
  x = dsp.Example(question=d.question, demos=dsp.all_but(train, d))
  x = program(x)

  SubmittedAnswer = dsp.Type(prefix="Submitted answer:", desc="${the submitted answer}")
  ExpertAnswer = dsp.Type(prefix="Expert answer:", desc="${the expert answer}")
  Response = dsp.Type(prefix="Response:", desc="${one of A, B, C, D, E}")

  approx_match_template = dsp.Template(
    instructions="""
    You are comparing a submitted answer to an expert answer on a given question. 
    Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.
    The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:
    (A) The submitted answer is a subset of the expert answer and is fully consistent with it.
    (B) The submitted answer is a superset of the expert answer and is fully consistent with it.
    (C) The submitted answer contains all the same details as the expert answer.
    (D) There is a disagreement between the submitted answer and the expert answer.
    (E) The answers differ, but these differences don't matter from the perspective of factuality.
    """, 
    submitted_answer=SubmittedAnswer(), 
    expert_answer=ExpertAnswer(),
    response=Response()
    )

  approx_match_example = dsp.Example(submitted_answer=x.answer, expert_answer=", ".join(d.answer), demos=[])
  _, completions = dsp.generate(approx_match_template)(approx_match_example, stage="lm_demo_eval")

  # print(x.answer)
  # print(", ".join(d.answer))
  # print(completions.response.strip().split()[0])

  if not EM(completions.response.strip().split()[0].upper(), ["A", "B", "C", "E"]): return None
  
  return d.copy(**x)


def answer_in_context(d, program, train):
  x = dsp.Example(question=d.question, demos=dsp.all_but(train, d))
  x = program(x)
  
  Question = dsp.Type(prefix="Question:", desc="${the question we are trying to answer}")
  GeneratedAnswer = dsp.Type(prefix="Generated answer:", desc="${the answer generated by the language model}")
  Context = dsp.Type(prefix="Generated context:", desc="${the context found by the retrieval model}")
  Response = dsp.Type(prefix="Response:", desc="${Whether the answer to the question is based on the context, either Yes or No}")

  answer_in_context_template = dsp.Template(
      instructions="Is the generated answer to the given question reasonable based on the context? Answer only Yes or No.",
      question=Question(),
      generated_answer=GeneratedAnswer(), 
      context=Context(),
      response=Response()
  )

  answer_in_context_example = dsp.Example(question=x.question, generated_answer=x.answer, context=x.context, demos=[])
  _, completions = dsp.generate(answer_in_context_template)(answer_in_context_example, stage="lm_demo_eval")

  if not EM(completions.response.strip().split()[0].lower(), ["yes"]): return None

  return d.copy(**x)


def ask_n_times(d, program, train, n=3):
  x = dsp.Example(question=d.question, demos=dsp.all_but(train, d))
  results = []
  for _ in range(n):
    output = program(x)
    results.append({
      "question": output.qa.question,
      "context": output.qa.context,
      "rationale": output.qa.rationale,
      "answer": output.qa.answer
    })
    # results.append(program(x))

  for item in results:
    print(item)
  
  # Question = dsp.Type(prefix="Question:", desc="${the question we are trying to answer}")
  # GeneratedAnswer = dsp.Type(prefix="Generated answer:", desc="${the answer generated by the language model}")
  # Context = dsp.Type(prefix="Generated context:", desc="${the context found by the retrieval model}")
  # Response = dsp.Type(prefix="Response:", desc="${Whether the answer to the question is based on the context, either Yes or No}")


  # answer_in_context_template = dsp.Template(
  #     instructions="Is the generated answer to the given question reasonable based on the context? Answer only Yes or No.",
  #     question=Question(),
  #     generated_answer=GeneratedAnswer(), 
  #     context=Context(),
  #     response=Response()
  # )

  # answer_in_context_example = dsp.Example(question=x.question, generated_answer=x.answer, context=x.context, demos=[])
  # _, completions = dsp.generate(answer_in_context_template)(answer_in_context_example, stage="lm_demo_eval")

  # if completions.response != "Yes": return None

  return d.copy(**x)
